{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoLFADS on Google Cloud Platform This tutorial is a programming-beginner friendly, step-by-step walkthrough on applying AutoLFADS, a deep learning tool that can be trained to uncover dynamics from single trial neural population data, using the computational resources of Google Cloud Platform. Quick introduction AutoLFADS is the combination of Latent Factor Analysis via Dynamical Systems (LFADS), a deep learning method to infer latent dynamics from single-trial neural population data, with Population Based Training (PBT), an automatic hyperparameter tuning framework. Specifically, this tutorial focuses on running AutoLFADS on Google Cloud Platform (GCP), which allows the use of AutoLFADS using computational resources available for rent on the cloud. Thus, as long as the user has access to GCP and the ability to pay for the usage of GPUs, then this tutorial can be used to apply AutoLFADS to neural population data without any need for local hardware. Requirements for this tutorial This tutorial is specifically designed for researchers and scientists with neural population data who are interested in using powerful deep-learning technology to uncover dynamics underlying neural populations. Fundamentally, this tutorial is designed so that significant programming knowledge or experience with deep-learning technology are NOT required. In order to use this tutorial, it is suggested that: Have neural population data in the format neurons x trial-length x number of trials , with sequence length <100 timesteps. Have access to Matlab and basic familiarity with it Have a Google account and can pay for the usage of GPUs on the Google Cloud platform. For an estimated rate, a 2.5 hour AutoLFADS run with 8 GPUs (example run in tutorial) will cost ~$7. For more detailed pricing on GPUs, go to https://cloud.google.com/compute/gpus-pricing . Requesting GPU quota Google Cloud Platform enforces a GPU quota to prevent unforseen spikes in usage. Requesting additional quota must be done 24-48 hours in advance of a run. If interested in running AutoLFADS through GCP, it is recommended to first request additional GPU quota. Instructions are detailed in the First Time Set-up section.","title":"Is This Tutorial For You?"},{"location":"#autolfads-on-google-cloud-platform","text":"This tutorial is a programming-beginner friendly, step-by-step walkthrough on applying AutoLFADS, a deep learning tool that can be trained to uncover dynamics from single trial neural population data, using the computational resources of Google Cloud Platform.","title":"AutoLFADS on Google Cloud Platform"},{"location":"#quick-introduction","text":"AutoLFADS is the combination of Latent Factor Analysis via Dynamical Systems (LFADS), a deep learning method to infer latent dynamics from single-trial neural population data, with Population Based Training (PBT), an automatic hyperparameter tuning framework. Specifically, this tutorial focuses on running AutoLFADS on Google Cloud Platform (GCP), which allows the use of AutoLFADS using computational resources available for rent on the cloud. Thus, as long as the user has access to GCP and the ability to pay for the usage of GPUs, then this tutorial can be used to apply AutoLFADS to neural population data without any need for local hardware.","title":"Quick introduction"},{"location":"#requirements-for-this-tutorial","text":"This tutorial is specifically designed for researchers and scientists with neural population data who are interested in using powerful deep-learning technology to uncover dynamics underlying neural populations. Fundamentally, this tutorial is designed so that significant programming knowledge or experience with deep-learning technology are NOT required. In order to use this tutorial, it is suggested that: Have neural population data in the format neurons x trial-length x number of trials , with sequence length <100 timesteps. Have access to Matlab and basic familiarity with it Have a Google account and can pay for the usage of GPUs on the Google Cloud platform. For an estimated rate, a 2.5 hour AutoLFADS run with 8 GPUs (example run in tutorial) will cost ~$7. For more detailed pricing on GPUs, go to https://cloud.google.com/compute/gpus-pricing .","title":"Requirements for this tutorial"},{"location":"#requesting-gpu-quota","text":"Google Cloud Platform enforces a GPU quota to prevent unforseen spikes in usage. Requesting additional quota must be done 24-48 hours in advance of a run. If interested in running AutoLFADS through GCP, it is recommended to first request additional GPU quota. Instructions are detailed in the First Time Set-up section.","title":"Requesting GPU quota"},{"location":"add_user/","text":"In this step, we add users to a Docker group in order to avoid prefacing docker commands with sudo . This step needs to be performed once per individual Google Account using AutoLFADS, before they start a run. Adding user to docker group First, navigate back to the compute engine and make sure the 'cloud shell' is open (button in the top right corner). Again, this step only needs to be done ONCE per Google account. Thus, if prior to this your Google account has already been adding to the docker group, this step does not need to be repeated. Furthermore, this step can only be done once the client machines are completely finished installing (remember to run check.sh to confirm this.) Once the cloud shell is open, run the following command sh add_docker_user.sh pbtclient . For this tutorial, the command would be sh add_docker_user.sh pbtclient Warning Leaving VMs running when not in use can lead to unintended high bills. Go to additional information section to see how to shut off unused VMs. Adding user to docker group walkthrough Pull AutoLFADS code onto server VM Next, we want to directly enter the server VM and copy the AutoLFADS code. In order to enter the server VM, we need to SSH in. First, make sure you're at the compute engine ( console.cloud.google.com/compute ) and can see your list of created VMs. Once you find your server you created (in this tutorial, its named tutserver), click on the button to the right of it labeled 'SSH'. You have now SSHed into your server client. Now, inside this newly opened SSH browser window (NOT the cloud shell), we want to clone the repository here in order to obtain the AutoLFADS python script. Run the following command in the SSH window. git clone https://github.com/snel-repo/autoLFADS-beta.git Now, the server VM should contain the necessary code. This completes creating all the necessary cloud infrastructure. Server Pull Code Walkthrough","title":"Set-up user"},{"location":"add_user/#adding-user-to-docker-group","text":"First, navigate back to the compute engine and make sure the 'cloud shell' is open (button in the top right corner). Again, this step only needs to be done ONCE per Google account. Thus, if prior to this your Google account has already been adding to the docker group, this step does not need to be repeated. Furthermore, this step can only be done once the client machines are completely finished installing (remember to run check.sh to confirm this.) Once the cloud shell is open, run the following command sh add_docker_user.sh pbtclient . For this tutorial, the command would be sh add_docker_user.sh pbtclient Warning Leaving VMs running when not in use can lead to unintended high bills. Go to additional information section to see how to shut off unused VMs.","title":"Adding user to docker group "},{"location":"add_user/#adding-user-to-docker-group-walkthrough","text":"","title":"Adding user to docker group walkthrough"},{"location":"add_user/#pull-autolfads-code-onto-server-vm","text":"Next, we want to directly enter the server VM and copy the AutoLFADS code. In order to enter the server VM, we need to SSH in. First, make sure you're at the compute engine ( console.cloud.google.com/compute ) and can see your list of created VMs. Once you find your server you created (in this tutorial, its named tutserver), click on the button to the right of it labeled 'SSH'. You have now SSHed into your server client. Now, inside this newly opened SSH browser window (NOT the cloud shell), we want to clone the repository here in order to obtain the AutoLFADS python script. Run the following command in the SSH window. git clone https://github.com/snel-repo/autoLFADS-beta.git Now, the server VM should contain the necessary code. This completes creating all the necessary cloud infrastructure.","title":"Pull AutoLFADS code onto server VM"},{"location":"add_user/#server-pull-code-walkthrough","text":"","title":"Server Pull Code Walkthrough"},{"location":"analysis/","text":"Info Estimated time for section: 30 min The following section details downloading the results from Google Cloud, and then running several analysis scripts. Downloading Data From GCP Once LFADS w/ PBT has finished running, we can now download the data back to our local computer for analysis. Warning At this point, you can stop all your machines (server and clients). Information on stopping machines can be found here To download data from the bucket, navigate back to https://console.cloud.google.com/storage . Click on the newly created zip file, and then click 'Download.' The output of the run should now be downloaded to your local computer. Post Processing Now that you have downloaded the data back to your local computer, open up the tutorial package ( download here ) Open up pbt_plot.m in Matlab. First, set the tutorial_package to your current working folder. Then, inside the pbt_plot.m script, we need to set the data_folder variable to the location of your pbt_run folder, which is located inside the run folder you downloaded. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\pbt_run . Set the output_folder variable to the folder where you want the plots generated. Run the pbt_plot.m script. This script will show the evolution of HPs over successive generations. These are some of the plots we got from the tutorial run. Compare to true rates Note This section is only for users using a synthetic dataset (as in the tutorial_package) where ground truth is available. If you used the synthetic dataset derived from Lorenz, then we can compare to the true rates. First, open the compare_rates.m script in the tutorial package. Fill in the first lfads_output_dir with the address of the lfads_output which is located inside your run folder. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\lfads_output . Then, you can run the script, which will generate R^2 value, which represents the error in the inferred rates compared to the true rates, as well as plot the inferred rates of several example neurons against their true underlying rates. The following plot was generated from this tutorial's run.","title":"Analysis"},{"location":"analysis/#downloading-data-from-gcp","text":"Once LFADS w/ PBT has finished running, we can now download the data back to our local computer for analysis. Warning At this point, you can stop all your machines (server and clients). Information on stopping machines can be found here To download data from the bucket, navigate back to https://console.cloud.google.com/storage . Click on the newly created zip file, and then click 'Download.' The output of the run should now be downloaded to your local computer.","title":"Downloading Data From GCP "},{"location":"analysis/#post-processing","text":"Now that you have downloaded the data back to your local computer, open up the tutorial package ( download here ) Open up pbt_plot.m in Matlab. First, set the tutorial_package to your current working folder. Then, inside the pbt_plot.m script, we need to set the data_folder variable to the location of your pbt_run folder, which is located inside the run folder you downloaded. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\pbt_run . Set the output_folder variable to the folder where you want the plots generated. Run the pbt_plot.m script. This script will show the evolution of HPs over successive generations. These are some of the plots we got from the tutorial run.","title":"Post Processing "},{"location":"analysis/#compare-to-true-rates","text":"Note This section is only for users using a synthetic dataset (as in the tutorial_package) where ground truth is available. If you used the synthetic dataset derived from Lorenz, then we can compare to the true rates. First, open the compare_rates.m script in the tutorial package. Fill in the first lfads_output_dir with the address of the lfads_output which is located inside your run folder. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\lfads_output . Then, you can run the script, which will generate R^2 value, which represents the error in the inferred rates compared to the true rates, as well as plot the inferred rates of several example neurons against their true underlying rates. The following plot was generated from this tutorial's run.","title":"Compare to true rates"},{"location":"architecture/","text":"Overview We will briefly describe the architecture of AutoLFADS, focusing on a high-level, theoretical overview over the basic organization and functionality of the various VMs created when beginning an AutoLFADS run. While this section describes the schema and theory behind the architecture of AutoLFADS, the steps to creating the basic architecture to begin an AutoLFADS runs are described beginning in first-time setup . Server and Client Server The architecture of AutoLFADS is largely centered on using several virtual machines (VMs), which we can imagine as emulations of real computers. In any AutoLFADS run, we first create a single server machine, which we can imagine as essentially 'overseeing' an AutoLFADS run. It triggers all the workers to begin training, oversees exploit/explore methodology, and receives various data back from the worker population. Client The next group of virtual machines we create are called 'client machines.' We can conceptualize the client machines as the VMs which actually run LFADS, which send and recieve information from the server machine. Each client machine has its own GPU and can run 3 processes at a time, in which each process can be used to train a worker. Using more client machines allows us to more quickly model our data when we want large numbers of workers. Docker, Shared Storage, and MongoDB While the interplay between server and client forms the backbone of an AutoLFADS run, there are various components that facilitate this. Docker The workers are run by client machines, but a LFADS worker generally requires a complex software setup. To avoid manual user set-up, each client machine has a Docker image with the required LFADS environment. For each client machine, each worker is run inside a Docker container. For more information on Docker, https://docs.docker.com/engine/docker-overview/ offers a more comprehensive overview. Shared Storage Shared storage contains the initial dataset AutoLFADS is analyzing, and throughout the run it stores the weights and HPs of each generation of worker. MongoDB We can imagine Mongo as facilitating the communication between server, clients, and shared storage. For instance, worker status is passed from the clients to server through Mongo.","title":"Architecture"},{"location":"architecture/#overview","text":"We will briefly describe the architecture of AutoLFADS, focusing on a high-level, theoretical overview over the basic organization and functionality of the various VMs created when beginning an AutoLFADS run. While this section describes the schema and theory behind the architecture of AutoLFADS, the steps to creating the basic architecture to begin an AutoLFADS runs are described beginning in first-time setup .","title":"Overview"},{"location":"architecture/#server-and-client","text":"","title":"Server and Client"},{"location":"architecture/#server","text":"The architecture of AutoLFADS is largely centered on using several virtual machines (VMs), which we can imagine as emulations of real computers. In any AutoLFADS run, we first create a single server machine, which we can imagine as essentially 'overseeing' an AutoLFADS run. It triggers all the workers to begin training, oversees exploit/explore methodology, and receives various data back from the worker population.","title":"Server"},{"location":"architecture/#client","text":"The next group of virtual machines we create are called 'client machines.' We can conceptualize the client machines as the VMs which actually run LFADS, which send and recieve information from the server machine. Each client machine has its own GPU and can run 3 processes at a time, in which each process can be used to train a worker. Using more client machines allows us to more quickly model our data when we want large numbers of workers.","title":"Client"},{"location":"architecture/#docker-shared-storage-and-mongodb","text":"While the interplay between server and client forms the backbone of an AutoLFADS run, there are various components that facilitate this.","title":"Docker, Shared Storage, and MongoDB"},{"location":"architecture/#docker","text":"The workers are run by client machines, but a LFADS worker generally requires a complex software setup. To avoid manual user set-up, each client machine has a Docker image with the required LFADS environment. For each client machine, each worker is run inside a Docker container. For more information on Docker, https://docs.docker.com/engine/docker-overview/ offers a more comprehensive overview.","title":"Docker"},{"location":"architecture/#shared-storage","text":"Shared storage contains the initial dataset AutoLFADS is analyzing, and throughout the run it stores the weights and HPs of each generation of worker.","title":"Shared Storage"},{"location":"architecture/#mongodb","text":"We can imagine Mongo as facilitating the communication between server, clients, and shared storage. For instance, worker status is passed from the clients to server through Mongo.","title":"MongoDB"},{"location":"check_run/","text":"Checking Run To confirm that LFADS w/ PBT is training, your tmux window should display some stage of the PBT process, either assigning workers or waiting for workers to finish training. Approximate Time For the sample Lorenz data, the entire process with 2 TPUs should take ~15 minutes. However, this is a small dataset -- larger datasets can take much longer, from hours to days.... (TO DO, give more concrete approximations) Stopping Run Early If you want to stop your run early, in your tmux window you can press ^c (ctrl-c) and then kill your tmux session. However, if a run is killed, you might not be able to start a run with th same VM (TO DO: find out if a run is killed, whether it can just be promptly restarted or new VM has to be created)_ Finished Run When training is finished, your tmux window should display the following. At this point, you're inferred rates is now located in the 'output' folder of your bucket. However, before we download the rates back to our local computer and begin analysis, we need to delete the VMs and TPUs as to not incur more costs. Warning Don't forget to delete your VMs and TPUs if you finished an LFADS w/ PBT run. Deleting VMs and TPUs To delete the server VMs and TPU, we can use the following command in the Cloud Shell (located in the upper right corner of the compute engine), and in the command replace SERVER_NAME, ZONE, NUM_TPUS with parameters set in the set-up script. curl -s https://raw.githubusercontent.com/snel-repo/test-repo/master/server-manager.sh | bash /dev/stdin delete <server-name> <zone> <num-tpu> This removes VMs and TPUs (as to not incur more charges once run is finished.) From here, we can now move on to the analysis section to download our data back from the cloud and process it.","title":"Check run"},{"location":"check_run/#checking-run","text":"To confirm that LFADS w/ PBT is training, your tmux window should display some stage of the PBT process, either assigning workers or waiting for workers to finish training.","title":"Checking Run"},{"location":"check_run/#approximate-time","text":"For the sample Lorenz data, the entire process with 2 TPUs should take ~15 minutes. However, this is a small dataset -- larger datasets can take much longer, from hours to days.... (TO DO, give more concrete approximations)","title":"Approximate Time"},{"location":"check_run/#stopping-run-early","text":"If you want to stop your run early, in your tmux window you can press ^c (ctrl-c) and then kill your tmux session. However, if a run is killed, you might not be able to start a run with th same VM (TO DO: find out if a run is killed, whether it can just be promptly restarted or new VM has to be created)_","title":"Stopping Run Early"},{"location":"check_run/#finished-run","text":"When training is finished, your tmux window should display the following. At this point, you're inferred rates is now located in the 'output' folder of your bucket. However, before we download the rates back to our local computer and begin analysis, we need to delete the VMs and TPUs as to not incur more costs. Warning Don't forget to delete your VMs and TPUs if you finished an LFADS w/ PBT run.","title":"Finished Run"},{"location":"check_run/#deleting-vms-and-tpus","text":"To delete the server VMs and TPU, we can use the following command in the Cloud Shell (located in the upper right corner of the compute engine), and in the command replace SERVER_NAME, ZONE, NUM_TPUS with parameters set in the set-up script. curl -s https://raw.githubusercontent.com/snel-repo/test-repo/master/server-manager.sh | bash /dev/stdin delete <server-name> <zone> <num-tpu> This removes VMs and TPUs (as to not incur more charges once run is finished.) From here, we can now move on to the analysis section to download our data back from the cloud and process it.","title":"Deleting VMs and TPUs"},{"location":"client_tmux/","text":"During a run, there is a variety of ways to check on the status of a run. SSHing into client VMs We can SSH into the client VMs in order to look at individual workers or look at client log files. This can be helpful as this is where possible error logs will be stored. We SSH into clients in the same way we SSH into servers. Simply navigate to the compute engine, find the client VM you wish to enter, and click 'SSH.' Enter docker container Once SSHed into a client machine, we can now enter the bash of the docker container, where we can view the individual workers running on tmux sessions or look at log files. This can be done by running the following command on the client machine. docker exec -it docker_pbt /bin/bash This command will make you enter the docker container running on the client VM. To view all tmux sessions, you can type the command. tmux -L pbt_server ls This lists out all the tmux sessions. You can view any of these tmux sessions by using the command. tmux -L pbt_server a -t <tmux_session_name> For more information on connecting to the Docker container in the client machines can be found at https://linuxize.com/post/how-to-connect-to-docker-container . Viewing log files Log files generated from each client machines can be useful to diagnose potential errors. These files are generated at the end of a run in your bucket located at console.cloud.google.com/storage . To view the log files, enter your run folder. There will be a client_logs folder which contains these log files. These files can also be viewed during the run by entering docker container and entering the temp folder with: cd /tmp Now with ls you can see the list of log files. When is run over? There are various conditions which would end a run, and in your server's tmux session, it should display the text seen in the following image:","title":"During run"},{"location":"client_tmux/#sshing-into-client-vms","text":"We can SSH into the client VMs in order to look at individual workers or look at client log files. This can be helpful as this is where possible error logs will be stored. We SSH into clients in the same way we SSH into servers. Simply navigate to the compute engine, find the client VM you wish to enter, and click 'SSH.'","title":"SSHing into client VMs"},{"location":"client_tmux/#enter-docker-container","text":"Once SSHed into a client machine, we can now enter the bash of the docker container, where we can view the individual workers running on tmux sessions or look at log files. This can be done by running the following command on the client machine. docker exec -it docker_pbt /bin/bash This command will make you enter the docker container running on the client VM. To view all tmux sessions, you can type the command. tmux -L pbt_server ls This lists out all the tmux sessions. You can view any of these tmux sessions by using the command. tmux -L pbt_server a -t <tmux_session_name> For more information on connecting to the Docker container in the client machines can be found at https://linuxize.com/post/how-to-connect-to-docker-container .","title":"Enter docker container"},{"location":"client_tmux/#viewing-log-files","text":"Log files generated from each client machines can be useful to diagnose potential errors. These files are generated at the end of a run in your bucket located at console.cloud.google.com/storage . To view the log files, enter your run folder. There will be a client_logs folder which contains these log files. These files can also be viewed during the run by entering docker container and entering the temp folder with: cd /tmp Now with ls you can see the list of log files.","title":"Viewing log files"},{"location":"client_tmux/#when-is-run-over","text":"There are various conditions which would end a run, and in your server's tmux session, it should display the text seen in the following image:","title":"When is run over?"},{"location":"cloud_infra/","text":"This section talks briefly about the cloud services which we will be employing. Note that this section is just a brief overview of running autoLFADS on the cloud, and step-by-step instructions on specifically running autoLFADS on GCP start can be found at First Time Set-up Google Cloud Platform (GCP) Overview Google Cloud Platform is a suite of cloud computing services, allowing users to access data storage, software, and computational power simply through internet connection. Running autoLFADS on GCP has several advantages over running it on local infrastructure, such as access to computational resources on demand. In order to use GCP, you need a Google Account, internet connection, and the ability to pay to rent GPUs. Note that an internet connection is necessary for all the steps to begin an autoLFADS run, but once the run has begun it will continue whether or not the user is connected to GCP. Why use GCP An autoLFADS run requires several VMs powered by GPUs, which can be expensive. Google Cloud Platform allows us to Google's computational resources and use them over the cloud, negating the ned for any sort of local hardware. Pipeline The general pipeline of running LFADS w/ PBT on GCP is described here. The high level features of the process are described below. We have tried to break down the process in the smallest constituents for the ease of understanding Create infrastructure The steps here are required to be performed once, to create the required infrastructure. It can be done by any user in the project Create the gcloud project with the resource quota for the required number of GPUs Create the server machine (it also hosts the mongo DB) Create the client machines (with all installations - nvidia-driver, docker images) Create storage buckets to store the raw data and trained models Set up infrastructure for the user The steps here are to be performed by each user, before they use PBT for the first time. It is to be done only once, for each user Add the user to the docker group on all client machines Executing PBT - These steps here are to be done before we run PBT Upload data to the shared storage and mount it on all client machines Start Docker on the client machines Run PBT In the following sections more details are provided on how to perform each of the above steps Further information on the architecture is elaborated in the Architecture section. Once LFADS w/ PBT is finished training, the estimated rates that come from LFADS can be downloaded back onto the local computer, and then processed and analyzed.","title":"Cloud infra"},{"location":"cloud_infra/#google-cloud-platform-gcp-overview","text":"Google Cloud Platform is a suite of cloud computing services, allowing users to access data storage, software, and computational power simply through internet connection. Running autoLFADS on GCP has several advantages over running it on local infrastructure, such as access to computational resources on demand. In order to use GCP, you need a Google Account, internet connection, and the ability to pay to rent GPUs. Note that an internet connection is necessary for all the steps to begin an autoLFADS run, but once the run has begun it will continue whether or not the user is connected to GCP.","title":"Google Cloud Platform (GCP) Overview"},{"location":"cloud_infra/#why-use-gcp","text":"An autoLFADS run requires several VMs powered by GPUs, which can be expensive. Google Cloud Platform allows us to Google's computational resources and use them over the cloud, negating the ned for any sort of local hardware.","title":"Why use GCP"},{"location":"cloud_infra/#pipeline","text":"The general pipeline of running LFADS w/ PBT on GCP is described here. The high level features of the process are described below. We have tried to break down the process in the smallest constituents for the ease of understanding","title":"Pipeline"},{"location":"cloud_infra/#create-infrastructure","text":"The steps here are required to be performed once, to create the required infrastructure. It can be done by any user in the project Create the gcloud project with the resource quota for the required number of GPUs Create the server machine (it also hosts the mongo DB) Create the client machines (with all installations - nvidia-driver, docker images) Create storage buckets to store the raw data and trained models","title":"Create infrastructure"},{"location":"cloud_infra/#set-up-infrastructure-for-the-user","text":"The steps here are to be performed by each user, before they use PBT for the first time. It is to be done only once, for each user Add the user to the docker group on all client machines","title":"Set up infrastructure for the user"},{"location":"cloud_infra/#executing-pbt-","text":"These steps here are to be done before we run PBT Upload data to the shared storage and mount it on all client machines Start Docker on the client machines Run PBT In the following sections more details are provided on how to perform each of the above steps Further information on the architecture is elaborated in the Architecture section. Once LFADS w/ PBT is finished training, the estimated rates that come from LFADS can be downloaded back onto the local computer, and then processed and analyzed.","title":"Executing PBT -"},{"location":"cloud_install/","text":"cloud install GPU vs TPU","title":"Cloud install"},{"location":"common_errors/","text":"Server Set-up error Several failures in the pipeline can be attributed to failure in server creation, which is done in the sh server_set_up.sh step. If the Python script fails to run, its likely worth first attempting to delete the server machine , and then re-run your server_set_up.sh script. After running the server_set_up.sh script, some of the last few lines of generated output should specify that the user has successfully been added to the MongoDB group, as seen in the below screenshot. If this isn't seen, likely MongoDB is failing to be set-up properly on the server machine. This can be checked by SSHing into the server machine and then running sudo service mongod status . Connection refused If you see a connection refused exit output after running the Python script, this might have to do with incorrect server set_up. While not exactly clear, it is likely worth checking that sh server_set_up.sh was successfully ran. Resource exhausted error If you see a resource exhausted error, usually following no response from process outputs, this might be due to having too large of a dataset. This tutorial recommends keeping the sequence length less than 100 timesteps.","title":"Common Errors"},{"location":"common_errors/#server-set-up-error","text":"Several failures in the pipeline can be attributed to failure in server creation, which is done in the sh server_set_up.sh step. If the Python script fails to run, its likely worth first attempting to delete the server machine , and then re-run your server_set_up.sh script. After running the server_set_up.sh script, some of the last few lines of generated output should specify that the user has successfully been added to the MongoDB group, as seen in the below screenshot. If this isn't seen, likely MongoDB is failing to be set-up properly on the server machine. This can be checked by SSHing into the server machine and then running sudo service mongod status .","title":"Server Set-up error"},{"location":"common_errors/#connection-refused","text":"If you see a connection refused exit output after running the Python script, this might have to do with incorrect server set_up. While not exactly clear, it is likely worth checking that sh server_set_up.sh was successfully ran.","title":"Connection refused"},{"location":"common_errors/#resource-exhausted-error","text":"If you see a resource exhausted error, usually following no response from process outputs, this might be due to having too large of a dataset. This tutorial recommends keeping the sequence length less than 100 timesteps.","title":"Resource exhausted error"},{"location":"create_bucket/","text":"Creating Bucket The next step is creating a bucket and adding two directories, one to upload your data to, and one to store checkpoints from your run. Navigate to console.cloud.google.com/storage and click 'Create Bucket.' The location type of the bucket should be \"multi-region,\" the storage class should be \"standard,\" and the access control should be \"fine-grained.\" After you have created your bucket, navigate inside your bucket by clicking on it. Inside your bucket, click \"create folder\" and name it \"data.\" Then create another folder called \"runs.\"","title":"Set-up storage infrastructure"},{"location":"create_bucket/#creating-bucket","text":"The next step is creating a bucket and adding two directories, one to upload your data to, and one to store checkpoints from your run. Navigate to console.cloud.google.com/storage and click 'Create Bucket.' The location type of the bucket should be \"multi-region,\" the storage class should be \"standard,\" and the access control should be \"fine-grained.\" After you have created your bucket, navigate inside your bucket by clicking on it. Inside your bucket, click \"create folder\" and name it \"data.\" Then create another folder called \"runs.\"","title":"Creating Bucket"},{"location":"create_infra/","text":"Info Estimated time for section: 45 minutes The following section details how to set-up cloud infrastructure on GCP. These steps need to be completed once per PROJECT. Set-up virtual machines Set-up storage infrastructure This step needs to be completed once per USER. That is, if a team is working on a project, each individual member with a Google Account has to execute the steps listed below. Set-up user Tutorial package This tutorial has a package that contains several scripts and a synthetic dataset useful for this tutorial. Note that depending on your specific needs, not every file in this package needs to be used. Download the tutorial package here . Create and configure the google cloud project This first step creates your project in Google Cloud Platform. This step only needs to be done once. Go to https://cloud.google.com/ and sign in with your Google account. If this is your first time logging into Google Cloud Platform, first we need to create a project. Navigate to the console.cloud.google.com/compute and then fill out the details needed to create a project. Next, you have to link your account to a billing account by entering a credit card. Navigate back to the dashboard at console.cloud.google.com/home and then click 'Billing,' located on the right side. Here you can add a method to pay for GPUs. Requesting additional GPU quota Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Google may take up to 48 hours to allocate quota (although generally much faster than this), so it is recommended to request additional quota well before planning to begin a run. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). On certain GCP accounts, this global GPU metric may not show up and if so, this specific quota can be ignored. Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater). On certain GCP accounts, this global CPU metric may not show up and if so, this specific quota can be ignored. Creating Server VM Once the project is created, we must create a server VM. To do so, first navigate back to the compute engine at console.cloud.google.com/compute . Then, we want to open the cloud shell by clicking the 'Activate Cloud Shell' button in the top right. Inside the shell of the gcloud project, we want to clone the SNEL repository. This has the collection of scripts which we will be using to create and set-up the server and client VMs and also to start PBT. git clone https://github.com/snel-repo/autoLFADS-beta.git Now, we want to navigate to the gcloud_scripts directory inside this repository. We can type the following command into our cloud shell to navigate to this directory. cd autoLFADS-beta/gcloud_scripts Next, we want to run the server_set_up.sh script, which has the following format: sh server_set_up.sh <server_name> <zone> Server_name is the name of the server we create, and can be any lowercase alphanumeric characters. Zone is the region where your server is stored and used; best is usually the zone closest to you (list of zones can be found here: https://cloud.google.com/compute/docs/regions-zones/#available ) In this tutorial, we'll create a server named 'tutserver' and in the zone 'us-central1-c'. We will thus execute the following command in the cloud shell. sh server_set_up.sh tutserver us-central1-c This step will take a couple minutes to run. You will know its complete when a server is seen in list of VMs and the compute engine shows the following. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section . Create Server VM Walkthrough If for some reason the server VM doesn't seem to have been correctly created, check the common errors section for debugging help. Create the client machines After creating the server machines, we next want to create the client machines. To do so, first make sure you have the cloud shell still opened and inside the directory autoLFADS-beta/gcloud_scripts . To create the client machines, we want to pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> <name_of_GPU> Client name is any lowercase alphanumeric characters naming the client, number_of_clients is how many client machines are created, and zone is the location the clients will be stored in. In order to decide how many clients to be created for your specific dataset, refer to additional information . Remember that the number of client machines is limited by your GPU quota in a particular zone, that is, your max number of GPUs in a zone is the max number of client machines you can create there . The argument only needs to be passed in if you are using a GPU that is not the default NVIDIA K80 GPU, and is the full name of the GPU without in lowercase with spaces replaced with dashes (for instance, nvidia-tesla-v100 ). In this tutorial, we will create 8 clients dispersed over two zones, as not to not exceed quota limits. Since we are using the NVIDIA K80 GPU we omit the <name_of_GPU> argument. To create these client machines in two zones, we run the following two lines of code consecutively. sh machine_setup.sh tutclientc 4 us-central1-c Once control of cloud shell is returned back: sh machine_setup.sh tutcliente 4 us-east1-c Create the client machines walkthrough Check if Docker is successfully installed Creating the clinet machines will take several minutes to run, and then an additional 10 or so minutes for the client machines to finish pulling the Docker image. Note that the creation of the client machines being finished is NOT indicated by control of Cloud Shell being returned or by there simply being a green checkmark next to the VMs; you must wait until the Docker image has been pulled on each machine. In order to see if the client machines have finished pulling the Docker image, run the following command in cloud shell. sh check.sh pbtclient Note pbtclient is the tagname, that is, a shorthand name for all client VMs in all zones. Commands that involve all client VMs at the same time, such as in the AutoLFADS python script and in check.sh, use pbtclient as the common client name. If the client machines are completely finished installing, then every client machine should return a Docker Installed on <client_name> Once all the machines have Docker properly installed, this marks the end of creating necessary VMs.","title":"Set-up virtual machines"},{"location":"create_infra/#tutorial-package","text":"This tutorial has a package that contains several scripts and a synthetic dataset useful for this tutorial. Note that depending on your specific needs, not every file in this package needs to be used. Download the tutorial package here .","title":"Tutorial package"},{"location":"create_infra/#create-and-configure-the-google-cloud-project","text":"This first step creates your project in Google Cloud Platform. This step only needs to be done once. Go to https://cloud.google.com/ and sign in with your Google account. If this is your first time logging into Google Cloud Platform, first we need to create a project. Navigate to the console.cloud.google.com/compute and then fill out the details needed to create a project. Next, you have to link your account to a billing account by entering a credit card. Navigate back to the dashboard at console.cloud.google.com/home and then click 'Billing,' located on the right side. Here you can add a method to pay for GPUs.","title":"Create and configure the google cloud project"},{"location":"create_infra/#requesting-additional-gpu-quota","text":"Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Google may take up to 48 hours to allocate quota (although generally much faster than this), so it is recommended to request additional quota well before planning to begin a run. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). On certain GCP accounts, this global GPU metric may not show up and if so, this specific quota can be ignored. Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater). On certain GCP accounts, this global CPU metric may not show up and if so, this specific quota can be ignored.","title":"Requesting additional GPU quota"},{"location":"create_infra/#creating-server-vm","text":"Once the project is created, we must create a server VM. To do so, first navigate back to the compute engine at console.cloud.google.com/compute . Then, we want to open the cloud shell by clicking the 'Activate Cloud Shell' button in the top right. Inside the shell of the gcloud project, we want to clone the SNEL repository. This has the collection of scripts which we will be using to create and set-up the server and client VMs and also to start PBT. git clone https://github.com/snel-repo/autoLFADS-beta.git Now, we want to navigate to the gcloud_scripts directory inside this repository. We can type the following command into our cloud shell to navigate to this directory. cd autoLFADS-beta/gcloud_scripts Next, we want to run the server_set_up.sh script, which has the following format: sh server_set_up.sh <server_name> <zone> Server_name is the name of the server we create, and can be any lowercase alphanumeric characters. Zone is the region where your server is stored and used; best is usually the zone closest to you (list of zones can be found here: https://cloud.google.com/compute/docs/regions-zones/#available ) In this tutorial, we'll create a server named 'tutserver' and in the zone 'us-central1-c'. We will thus execute the following command in the cloud shell. sh server_set_up.sh tutserver us-central1-c This step will take a couple minutes to run. You will know its complete when a server is seen in list of VMs and the compute engine shows the following. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section .","title":"Creating Server VM "},{"location":"create_infra/#create-server-vm-walkthrough","text":"If for some reason the server VM doesn't seem to have been correctly created, check the common errors section for debugging help.","title":"Create Server VM Walkthrough"},{"location":"create_infra/#create-the-client-machines","text":"After creating the server machines, we next want to create the client machines. To do so, first make sure you have the cloud shell still opened and inside the directory autoLFADS-beta/gcloud_scripts . To create the client machines, we want to pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> <name_of_GPU> Client name is any lowercase alphanumeric characters naming the client, number_of_clients is how many client machines are created, and zone is the location the clients will be stored in. In order to decide how many clients to be created for your specific dataset, refer to additional information . Remember that the number of client machines is limited by your GPU quota in a particular zone, that is, your max number of GPUs in a zone is the max number of client machines you can create there . The argument only needs to be passed in if you are using a GPU that is not the default NVIDIA K80 GPU, and is the full name of the GPU without in lowercase with spaces replaced with dashes (for instance, nvidia-tesla-v100 ). In this tutorial, we will create 8 clients dispersed over two zones, as not to not exceed quota limits. Since we are using the NVIDIA K80 GPU we omit the <name_of_GPU> argument. To create these client machines in two zones, we run the following two lines of code consecutively. sh machine_setup.sh tutclientc 4 us-central1-c Once control of cloud shell is returned back: sh machine_setup.sh tutcliente 4 us-east1-c","title":"Create the client machines "},{"location":"create_infra/#create-the-client-machines-walkthrough","text":"","title":"Create the client machines walkthrough"},{"location":"create_infra/#check-if-docker-is-successfully-installed","text":"Creating the clinet machines will take several minutes to run, and then an additional 10 or so minutes for the client machines to finish pulling the Docker image. Note that the creation of the client machines being finished is NOT indicated by control of Cloud Shell being returned or by there simply being a green checkmark next to the VMs; you must wait until the Docker image has been pulled on each machine. In order to see if the client machines have finished pulling the Docker image, run the following command in cloud shell. sh check.sh pbtclient Note pbtclient is the tagname, that is, a shorthand name for all client VMs in all zones. Commands that involve all client VMs at the same time, such as in the AutoLFADS python script and in check.sh, use pbtclient as the common client name. If the client machines are completely finished installing, then every client machine should return a Docker Installed on <client_name> Once all the machines have Docker properly installed, this marks the end of creating necessary VMs.","title":"Check if Docker is successfully installed"},{"location":"data/","text":"Info Estimated time for section: 2 hours This section details the steps for connecting your dataset to AutoLFADS and then beginning a run. The following steps have to be done everytime you want to begin an AutoLFADS run with a new dataset. If you already have data uploaded to the bucket, and you have already linked your run parameters , then you can proceed straight to the run autoLFADS section. At this point in the tutorial, you have already set up your cloud infrastructure . If you would like to add additional client machines, the way to do this is explained in add additional client machines section in the 'Additional Information' section. Note, if you are re-using client machines you have created some time ago, there is a chance you might have to update your Docker image on all client machines. Instructions for doing so are in the update Docker image section. Setting up your data Sample dataset This tutorial has a sample synthetic dataset derived from a Lorenz system that can be used to set-up an AutoLFADS run. This synthetic dataset is ideal to test out AutoLFADS as it is intrinsically low-D, and the inferred rates generated by AutoLFADS can easily be compared to its true rates to give a sense of the effectiveness of AutoLFADS. The synthetic dataset is located in the tutorial package, called lfads_data.h5 . If you have not downloaded it yet, you can download it here . If you would like to use this dataset, you can skip the next section on using your own data, and proceed straight to the 'Uploading data' section in this page. Using your own neural population data If you would like to use your own neuronal population data, the data must be a .mat file with a .spike attribute which contains spiking data in the format neurons x trial-length x number of trials . Furthermore, we suggest using data with a sequence length less than 100 timesteps; while there is not a clear data size limit, larger datasets are more prone to failure when running the python script. Once you have your dadta in a .mat file, you can use the convert_h5.m in the script zip file to convert it to the necessary .h5 file format in order to upload it to Google Cloud Platform. Simply open up the script, point the data field toward your .mat file and edit the lfads_input_file to point toward where you want the .h5 file to be generated. Optionally, you can also edit the valid_set_ratio variable to use a different amount of the data for validation. Once you have set these variables, you can run the script and it should output the converted .h5 file. Uploading Data Next, navigate back to console.cloud.google.com/storage , click on the bucket you created , and then navigate to your \"data\" folder. Then, upload your .h5 file here either by clicking upload file or by dragging and dropping. The .h5 file must be renamed to have the prefix specified by the data_file_namestem parameter in the pbt_script_multiVM.py file located in autoLFADS_beta/pbt_opt directory of the SNEL repo. The default prefix is lfads , thus, in this tutorial we'll upload our data named lfads_data.h5. Our data folder in our bucket should now look like this with the data uploaded.","title":"Upload data"},{"location":"data/#setting-up-your-data","text":"","title":"Setting up your data"},{"location":"data/#sample-dataset","text":"This tutorial has a sample synthetic dataset derived from a Lorenz system that can be used to set-up an AutoLFADS run. This synthetic dataset is ideal to test out AutoLFADS as it is intrinsically low-D, and the inferred rates generated by AutoLFADS can easily be compared to its true rates to give a sense of the effectiveness of AutoLFADS. The synthetic dataset is located in the tutorial package, called lfads_data.h5 . If you have not downloaded it yet, you can download it here . If you would like to use this dataset, you can skip the next section on using your own data, and proceed straight to the 'Uploading data' section in this page.","title":"Sample dataset"},{"location":"data/#using-your-own-neural-population-data","text":"If you would like to use your own neuronal population data, the data must be a .mat file with a .spike attribute which contains spiking data in the format neurons x trial-length x number of trials . Furthermore, we suggest using data with a sequence length less than 100 timesteps; while there is not a clear data size limit, larger datasets are more prone to failure when running the python script. Once you have your dadta in a .mat file, you can use the convert_h5.m in the script zip file to convert it to the necessary .h5 file format in order to upload it to Google Cloud Platform. Simply open up the script, point the data field toward your .mat file and edit the lfads_input_file to point toward where you want the .h5 file to be generated. Optionally, you can also edit the valid_set_ratio variable to use a different amount of the data for validation. Once you have set these variables, you can run the script and it should output the converted .h5 file.","title":"Using your own neural population data"},{"location":"data/#uploading-data","text":"Next, navigate back to console.cloud.google.com/storage , click on the bucket you created , and then navigate to your \"data\" folder. Then, upload your .h5 file here either by clicking upload file or by dragging and dropping. The .h5 file must be renamed to have the prefix specified by the data_file_namestem parameter in the pbt_script_multiVM.py file located in autoLFADS_beta/pbt_opt directory of the SNEL repo. The default prefix is lfads , thus, in this tutorial we'll upload our data named lfads_data.h5. Our data folder in our bucket should now look like this with the data uploaded.","title":"Uploading Data"},{"location":"example/","text":"will add at end once PBT on TPU is up and running Here, we will show a brief summary of an entire LFADS w/ PBT run, including the expected input, the process of using Google Cloud Platform, and the inferred rates we extract out of it. First we expect data in this format PICTURE OF DATA Here, we're uploading data to the cloud and running it on the cloud shell. PIC Here we are downloading posterior rates PIC Here is some analyis and graphs. PIC","title":"Example"},{"location":"flowchart/","text":"Overview The following quick start guide offers a general pipeline for the steps of running AutoLFADS over Google Cloud Platform, and each step can be expanded to offer abbreviated instructions including code and links to detailed sections. Brand new users are recommended to use the more detailed versions of instructions, beginning with First Time Set-up . (First run begin here) STEP 1 - Set Up Project 10 Minutes 1.1: Create project, set up billing 1.2: Request additional quota (if needed) Up to 48 Hour Wait STEP 2 - Set-up Infrastructure 45 Minutes 2.1: Clone SNEL repository into cloud shell (Code in red border should be executed in cloud shell) git clone -b GCP https://github.com/snel-repo/autolfads.git 2.2: Create server machine sh server_set_up.sh tutserver us-central1-c Up to 10 Minute Wait 2.3: Create client machines sh machine_setup.sh tutcliente 4 us-east1-c sh machine_setup.sh tutclientc 4 us-central1-c Up to 20 Minute Wait 2.4: Check if Docker is finished installing sucessfully sh check.sh pbtclient 2.5: Create bucket with folders data and run (New users to already existing project begin here) STEP 3 - Add User 5 Minutes (This step can only be done after Docker is finished installing) 3.1: Add user to Docker group sh add_docker_user.sh pbtclient 3.2: Clone SNEL repository on server (Code with blue border should be executed in server shell) git clone -b GCP https://github.com/snel-repo/autolfads.git (Additional run, new dataset begin here) STEP 4 - Upload Data 5 Minutes 4.1: Upload data with prefix lfads (Additional run, same dataset begin here) STEP 5 - Start Run 10 Minutes 5.1: Link pbt_script_multiVM.py to your data and edit parameters 5.2: Begin AutoLFADS run in tmux tmux python2 pbt_script_multiVM.py Up to 2 Hour Wait STEP 6 - Analyze Output 30 Minutes (You should stop all machines now) 6.1: Download output zip file from bucket 6.2: Run pbt_plot.m","title":"Quick Start"},{"location":"flowchart/#overview","text":"The following quick start guide offers a general pipeline for the steps of running AutoLFADS over Google Cloud Platform, and each step can be expanded to offer abbreviated instructions including code and links to detailed sections. Brand new users are recommended to use the more detailed versions of instructions, beginning with First Time Set-up . (First run begin here) STEP 1 - Set Up Project 10 Minutes 1.1: Create project, set up billing 1.2: Request additional quota (if needed) Up to 48 Hour Wait STEP 2 - Set-up Infrastructure 45 Minutes 2.1: Clone SNEL repository into cloud shell (Code in red border should be executed in cloud shell) git clone -b GCP https://github.com/snel-repo/autolfads.git 2.2: Create server machine sh server_set_up.sh tutserver us-central1-c Up to 10 Minute Wait 2.3: Create client machines sh machine_setup.sh tutcliente 4 us-east1-c sh machine_setup.sh tutclientc 4 us-central1-c Up to 20 Minute Wait 2.4: Check if Docker is finished installing sucessfully sh check.sh pbtclient 2.5: Create bucket with folders data and run (New users to already existing project begin here) STEP 3 - Add User 5 Minutes (This step can only be done after Docker is finished installing) 3.1: Add user to Docker group sh add_docker_user.sh pbtclient 3.2: Clone SNEL repository on server (Code with blue border should be executed in server shell) git clone -b GCP https://github.com/snel-repo/autolfads.git (Additional run, new dataset begin here) STEP 4 - Upload Data 5 Minutes 4.1: Upload data with prefix lfads (Additional run, same dataset begin here) STEP 5 - Start Run 10 Minutes 5.1: Link pbt_script_multiVM.py to your data and edit parameters 5.2: Begin AutoLFADS run in tmux tmux python2 pbt_script_multiVM.py Up to 2 Hour Wait STEP 6 - Analyze Output 30 Minutes (You should stop all machines now) 6.1: Download output zip file from bucket 6.2: Run pbt_plot.m","title":"Overview"},{"location":"glossary/","text":"ogle cloud related parameters used in shell scripts to set up cloud infrastructure Shell scripts server_name - Name of the server VM. Used in the server_set_up.sh script zone - zone in which the VM will be created. Used for both - server and the client VMs. Used in the server_set_up.sh and machine_setup.sh scripts client_name - The root name of the client VM created when you call machine_setup.sh script. When you create 3 client VMs with client_name set to \"clvm\", the client machines are named clvm1, clvm2 clvm3. Note that this is different from the tag name. Used in the machine_setup.sh script number_of_clients - Number of client machines created when you call the machine_setup.sh script. Each client VM has a single GPU name_of_GPU - The GPU used with each client machine. The default GPU is nvidia-tesla-k80 tagname - this is the common alias of all client VMs. Helps in cases when a given command is needed to be run on all client VMs. We have set the tagname to \"pbtclient\" and this need not be changed PBT PARAMS pbt script bucket_name - Name of the cloud bucket data_path - data directory inside the cloud bucket run_path - run directory inside the cloud bucket nprocess_gpu - Number of processes to be run on each GPU Server Object epochs_per_generation - Number of epochs per each generation max_generations - Maximum number of generations. It may actually take lesser generations to converge to the best model, depending on the converging criterion defined in 'num_no_best_to_stop' and 'min_change_to_stop' explore_method - The method used to explore hyper-parameters. Accepts two possible arguments - 'perturb' and 'resample' explore_param - The parameters for the explore method defined by the 'explore_method' arg. When the 'explore_method' is set to 'perturb', the explore_param takes in a scalar between 0 and 1. A random number is then sampled uniformly from (1-explore_parama, 1+explore_param). This sampled number is then used as a factor which is multiplied to the HPs current value to perturb it. When the 'explore_method' is set to 'resample', the 'explore_param' must be set to None. In this condition, the new value of the HP is then obtained by resampling from a range of values defined for that parameter (Defined in the 'add_hp' method) mongo_server_ip : The ip address of the VM on which mongo DB is running. By default, the mongo DB runs on the server and the ip address of the server is passed her port : The port on which mongo DB runs (on the machine identified by mongo_server_ip) server_log_path - The path where the server generated log files are stored. Not advised to change this parameter num_no_best_to_stop : If no improvement is seen in the best worker for these many successive generations, PBT is terminated even before the max_generations are completed min_change_to_stop : If the improvement in the best worker over successive generations is less than this quantity, the improvement is considered to be 0. Default value is 0.0005 (or .05 percentage ). If the num_no_best_to_stop is set to 5 and min_change_to_stop is set to 0.0005 , the PBT training will terminate if for 5 successive generations the improvement in the best worker is less than 0.05% over the previous best worker docker_name : The name of the docker container. By default set to \"docker_pbt\" in the pbt_helper_fn.py details for the add_hp method name : name of the HP value - uses tuple to indicate range, or list/nparray for allowable values init_sample_mode: 'rand','logrand' or 'grid', mode of preferred initialization. Or pass explorable: True or False, whether the 'explore' action should apply to this hyperparameter explore_method : Same as the 'explore_method' defined for the Server class. The value of the 'explore_method' defined for the Server class is the default 'explore_method' for all HPs. However this value can be overwritten by passing it again for the specific HP. explore_param : Same as the 'explore_param' defined for the Server class. This can be overwritten for a specific HP just like explore_method limit_explore : If True, the new value of the HP after applying the explore method, cannot exceed the range defined by the \"value\" arg. LFADS keep_prob - Dropout keep probability keep_ratio - coordinated dropout input keep probability l2_gen_scale - L2 regularization cost for the generator l2_ic_enc_scale - L2 regularization cost for the initial condition encoder l2_con_scale - L2 cost for the controller l2_ci_enc_scale - L2 cost for the controller input encoder kl_co_weight - Strength of KL weight on controller output KL penalty kl_ic_weight - Strength of KL weight on initial conditions KL penalty do_calc_r2 - Calculate R^2 if the truth rates are available cell_clip_value - Max value recurrent cell can take before being clipped prior_ar_atau - Initial autocorrelation of AR(1) priors. This param is not searched often prior_ar_nvar - Initial noise variance for AR(1) priors. This param is not searched often ckpt_save_interval - Number of epochs between saving (non-lve) checkpoints. Doesn't has to be searched do_train_prior_ar_atau - Determines whether the value for atau is trainable, or not. Boolean do_train_prior_ar_nvar - Determines whether the value for noise variance is trainable, or not. Boolean kl_start_epoch - Start increasing KL weight after these many epochs l2_start_epoch - Start increasing l2 weight after these many epochs kl_increase_epochs - Increase KL weight for these many epochs l2_increase_epochs - Increase l2 weight for these many epochs batch_size - batch_size to use during training valid_batch_size - batch_size to use during validation val_cost_for_pbt - Set to either held-out samples (\"heldout_samp\") or heldout trials (\"heldout_trial\"). Validation cost is computed over these cv_keep_ratio - Cross-validation keep probability. Ratio of samples kept for training in the train set - if set to 80%, then 20% of samples from the training set are used for sample validation cd_grad_passthru_prob - Probability of passing through gradients in coordinated dropout. Allows some percentage of gradients to backpropagate - if set to 0.1 , then 10% of gradients which were supposed to be blocked, are actually passed through factors_dim - Number of factors from the generator ic_enc_dim - dimension of hidden state of the initial condition encoder ic_enc_seg_len - Segment length passed to initial condition encoder for causal modeling. Set to 0 (default) gen_dim - size of hidden state for generator co_dim - dimensionality of the inferred inputs by the controller ci_enc_dim - size of the hidden state in the controller encoder con_dim - \"Cell hidden size, controller\" - hidden state of the controller do_causal_controller - Restrict the controller to infer only causal inputs. Boolean output_dist - spikes are modeled as observations of underlying rates, modeled as this distribution. Default - 'poisson' learning_rate_decay_factor - Learning rate decay, decay by this fraction (How frequently is the decay applied) learning_rate_stop - stop training when the learning rate reaches this value learning_rate_n_to_compare - The current cost has to be less than these many previous costs to lower learning rate checkpoint_pb_load_name - Name of checkpoint files. Default - 'checkpoint' loss_scale - scaling of loss adam_epsilon - Epsilon parameter for Adam optimizer beta1 - beta1 parameter for Adam optimizer beta2 - beta2 parameter for Adam optimizer data_filename_stem - prefix for the data filename (h5 file) data_dir - directory of the data h5 file do_train_readin - Whether to train the read-in matrices and bias vectors. Boolean. False - leave them fixed at their initial values specified by the alignment matrices and vectors do_train_encoder_only - Train only the encoder weights cv_rand_seed - Random seed for held-out cross-validation sample mask output_filename_stem - Name of output file (postfix will be added) max_ckpt_to_keep - Max number of checkpoints to keep (keeps that many latest checkpoints) max_ckpt_to_keep_lve - Max number of checkpoints to keep for lowest validation error models (keeps that many lowest validation error checkpoints) csv_log - Name of file to keep the log of fit likelihoods (.csv appended to name) checkpoint_name - Name of checkpoint files (.ckpt appended) device - which device to use (GPU/CPU). By default set to GPU. ps_nexamples_to_process - Number of examples to process for posterior sample and average (not number of samples to average over) ic_prior_var - Minimum variance of IC prior distribution ic_post_var_min - Minimum variance of IC posterior distribution co_prior_var - Variance of controller input prior distribution do_feed_factors_to_controller - Should the controller network receive the feedback from the factors. Boolean. Should be set to True temporal_spike_jitter_width - jitters the spike, adds temporal noise during training. Avoids overfitting individual spikes inject_ext_input_to_gen - Inject the external input to the generator (Boolean). Should be set to True allow_gpu_growth - If true, only allocate the amount of memory needed for Session. Otherwise, use full GPU memory. Boolean max_grad_norm - Maximum norm of gradient before gradient clipping is applied do_reset_learning_rate - Reset the learning rate to initial value from the provided HP (HP - 'learning_rate_init'). Should be set to True","title":"Glossary"},{"location":"glossary/#shell-scripts","text":"server_name - Name of the server VM. Used in the server_set_up.sh script zone - zone in which the VM will be created. Used for both - server and the client VMs. Used in the server_set_up.sh and machine_setup.sh scripts client_name - The root name of the client VM created when you call machine_setup.sh script. When you create 3 client VMs with client_name set to \"clvm\", the client machines are named clvm1, clvm2 clvm3. Note that this is different from the tag name. Used in the machine_setup.sh script number_of_clients - Number of client machines created when you call the machine_setup.sh script. Each client VM has a single GPU name_of_GPU - The GPU used with each client machine. The default GPU is nvidia-tesla-k80 tagname - this is the common alias of all client VMs. Helps in cases when a given command is needed to be run on all client VMs. We have set the tagname to \"pbtclient\" and this need not be changed","title":"Shell scripts"},{"location":"glossary/#pbt-params","text":"","title":"PBT PARAMS"},{"location":"glossary/#pbt-script","text":"bucket_name - Name of the cloud bucket data_path - data directory inside the cloud bucket run_path - run directory inside the cloud bucket nprocess_gpu - Number of processes to be run on each GPU","title":"pbt script"},{"location":"glossary/#server-object","text":"epochs_per_generation - Number of epochs per each generation max_generations - Maximum number of generations. It may actually take lesser generations to converge to the best model, depending on the converging criterion defined in 'num_no_best_to_stop' and 'min_change_to_stop' explore_method - The method used to explore hyper-parameters. Accepts two possible arguments - 'perturb' and 'resample' explore_param - The parameters for the explore method defined by the 'explore_method' arg. When the 'explore_method' is set to 'perturb', the explore_param takes in a scalar between 0 and 1. A random number is then sampled uniformly from (1-explore_parama, 1+explore_param). This sampled number is then used as a factor which is multiplied to the HPs current value to perturb it. When the 'explore_method' is set to 'resample', the 'explore_param' must be set to None. In this condition, the new value of the HP is then obtained by resampling from a range of values defined for that parameter (Defined in the 'add_hp' method) mongo_server_ip : The ip address of the VM on which mongo DB is running. By default, the mongo DB runs on the server and the ip address of the server is passed her port : The port on which mongo DB runs (on the machine identified by mongo_server_ip) server_log_path - The path where the server generated log files are stored. Not advised to change this parameter num_no_best_to_stop : If no improvement is seen in the best worker for these many successive generations, PBT is terminated even before the max_generations are completed min_change_to_stop : If the improvement in the best worker over successive generations is less than this quantity, the improvement is considered to be 0. Default value is 0.0005 (or .05 percentage ). If the num_no_best_to_stop is set to 5 and min_change_to_stop is set to 0.0005 , the PBT training will terminate if for 5 successive generations the improvement in the best worker is less than 0.05% over the previous best worker docker_name : The name of the docker container. By default set to \"docker_pbt\" in the pbt_helper_fn.py details for the add_hp method name : name of the HP value - uses tuple to indicate range, or list/nparray for allowable values init_sample_mode: 'rand','logrand' or 'grid', mode of preferred initialization. Or pass explorable: True or False, whether the 'explore' action should apply to this hyperparameter explore_method : Same as the 'explore_method' defined for the Server class. The value of the 'explore_method' defined for the Server class is the default 'explore_method' for all HPs. However this value can be overwritten by passing it again for the specific HP. explore_param : Same as the 'explore_param' defined for the Server class. This can be overwritten for a specific HP just like explore_method limit_explore : If True, the new value of the HP after applying the explore method, cannot exceed the range defined by the \"value\" arg.","title":"Server Object"},{"location":"glossary/#lfads","text":"keep_prob - Dropout keep probability keep_ratio - coordinated dropout input keep probability l2_gen_scale - L2 regularization cost for the generator l2_ic_enc_scale - L2 regularization cost for the initial condition encoder l2_con_scale - L2 cost for the controller l2_ci_enc_scale - L2 cost for the controller input encoder kl_co_weight - Strength of KL weight on controller output KL penalty kl_ic_weight - Strength of KL weight on initial conditions KL penalty do_calc_r2 - Calculate R^2 if the truth rates are available cell_clip_value - Max value recurrent cell can take before being clipped prior_ar_atau - Initial autocorrelation of AR(1) priors. This param is not searched often prior_ar_nvar - Initial noise variance for AR(1) priors. This param is not searched often ckpt_save_interval - Number of epochs between saving (non-lve) checkpoints. Doesn't has to be searched do_train_prior_ar_atau - Determines whether the value for atau is trainable, or not. Boolean do_train_prior_ar_nvar - Determines whether the value for noise variance is trainable, or not. Boolean kl_start_epoch - Start increasing KL weight after these many epochs l2_start_epoch - Start increasing l2 weight after these many epochs kl_increase_epochs - Increase KL weight for these many epochs l2_increase_epochs - Increase l2 weight for these many epochs batch_size - batch_size to use during training valid_batch_size - batch_size to use during validation val_cost_for_pbt - Set to either held-out samples (\"heldout_samp\") or heldout trials (\"heldout_trial\"). Validation cost is computed over these cv_keep_ratio - Cross-validation keep probability. Ratio of samples kept for training in the train set - if set to 80%, then 20% of samples from the training set are used for sample validation cd_grad_passthru_prob - Probability of passing through gradients in coordinated dropout. Allows some percentage of gradients to backpropagate - if set to 0.1 , then 10% of gradients which were supposed to be blocked, are actually passed through factors_dim - Number of factors from the generator ic_enc_dim - dimension of hidden state of the initial condition encoder ic_enc_seg_len - Segment length passed to initial condition encoder for causal modeling. Set to 0 (default) gen_dim - size of hidden state for generator co_dim - dimensionality of the inferred inputs by the controller ci_enc_dim - size of the hidden state in the controller encoder con_dim - \"Cell hidden size, controller\" - hidden state of the controller do_causal_controller - Restrict the controller to infer only causal inputs. Boolean output_dist - spikes are modeled as observations of underlying rates, modeled as this distribution. Default - 'poisson' learning_rate_decay_factor - Learning rate decay, decay by this fraction (How frequently is the decay applied) learning_rate_stop - stop training when the learning rate reaches this value learning_rate_n_to_compare - The current cost has to be less than these many previous costs to lower learning rate checkpoint_pb_load_name - Name of checkpoint files. Default - 'checkpoint' loss_scale - scaling of loss adam_epsilon - Epsilon parameter for Adam optimizer beta1 - beta1 parameter for Adam optimizer beta2 - beta2 parameter for Adam optimizer data_filename_stem - prefix for the data filename (h5 file) data_dir - directory of the data h5 file do_train_readin - Whether to train the read-in matrices and bias vectors. Boolean. False - leave them fixed at their initial values specified by the alignment matrices and vectors do_train_encoder_only - Train only the encoder weights cv_rand_seed - Random seed for held-out cross-validation sample mask output_filename_stem - Name of output file (postfix will be added) max_ckpt_to_keep - Max number of checkpoints to keep (keeps that many latest checkpoints) max_ckpt_to_keep_lve - Max number of checkpoints to keep for lowest validation error models (keeps that many lowest validation error checkpoints) csv_log - Name of file to keep the log of fit likelihoods (.csv appended to name) checkpoint_name - Name of checkpoint files (.ckpt appended) device - which device to use (GPU/CPU). By default set to GPU. ps_nexamples_to_process - Number of examples to process for posterior sample and average (not number of samples to average over) ic_prior_var - Minimum variance of IC prior distribution ic_post_var_min - Minimum variance of IC posterior distribution co_prior_var - Variance of controller input prior distribution do_feed_factors_to_controller - Should the controller network receive the feedback from the factors. Boolean. Should be set to True temporal_spike_jitter_width - jitters the spike, adds temporal noise during training. Avoids overfitting individual spikes inject_ext_input_to_gen - Inject the external input to the generator (Boolean). Should be set to True allow_gpu_growth - If true, only allocate the amount of memory needed for Session. Otherwise, use full GPU memory. Boolean max_grad_norm - Maximum norm of gradient before gradient clipping is applied do_reset_learning_rate - Reset the learning rate to initial value from the provided HP (HP - 'learning_rate_init'). Should be set to True","title":"LFADS"},{"location":"installation/","text":"Installation These instructions will walk you through installing lfads, pbt, and connecting to google cloud platofrm. Installing TensorFlow .. Installing LFADS .. Installing PBT .. mongoDB?? Other stuff?? ..","title":"Installation"},{"location":"installation/#installation","text":"These instructions will walk you through installing lfads, pbt, and connecting to google cloud platofrm.","title":"Installation"},{"location":"installation/#installing-tensorflow","text":"..","title":"Installing TensorFlow"},{"location":"installation/#installing-lfads","text":"..","title":"Installing LFADS"},{"location":"installation/#installing-pbt","text":"..","title":"Installing PBT"},{"location":"installation/#mongodb-other-stuff","text":"..","title":"mongoDB?? Other stuff??"},{"location":"introductionAddInfo/","text":"Binary Tournament Binary tournament is the algorithm by which different workers are compared against each other during 'exploit' phases.","title":"Additional Info"},{"location":"introductionAddInfo/#binary-tournament","text":"Binary tournament is the algorithm by which different workers are compared against each other during 'exploit' phases.","title":"Binary Tournament"},{"location":"log_files/","text":"Log files available after a run contain a variety of information on the run, including hyperparameter evolution, best workers, etc. This section briefly explores how to find and use these files. Where to find log files Log files will show up in the bucket you created. Then, enter the runs folder, and navigate to the created pbt_run folder. Inside here, there exist two main types of log files, the first created from the server machine, and the second created from individual worker runs. Server log files Immediately inside the pbt_run folder are the log files created by the server. They contain many important information related to overall output of the run. Important log files include: best_worker.sofar , which lists the best worker until that point in the run. best_worker.done , which lists the best overall worker once the run is complete. decision_log.csv , which shows the result of binary tournament for all workers in all generations. For more information on binary tournament, refer to the binary tournament section. hp_log.csv , which shows the evolution of HPs across all workers and all generations. Worker log files Inside the pbt_run folder, there exist many folders to reach the specific worker in a specific generation, which follow the format gX_WY , or Generation X, Worker Y. For instance, generation 3 worker 4 would have a full named g003_w04 . We can navigate inside these specific workers folders to look at a variety of log files related to a specific worker's output. These files include: hp_history.csv , a history of the hyper-parameters for that worker (shows the HP evolution for the particular worker) gradnorms.csv , the norm of the gradient in each epoch in that generation, for the worker perf_history.csv , the performance history over all completed generations for that particular worker hyperparameters.txt , the hyperparameters for the worker for that generation fitlog.csv , the training and validation loss and various model checkpoint files Client log files Client log files log the output from each process, and any errors within the client will be recorded here. To access these, go to the client_log folder which is inside the run folder. This is available only when the run is over.","title":"Log Files"},{"location":"log_files/#where-to-find-log-files","text":"Log files will show up in the bucket you created. Then, enter the runs folder, and navigate to the created pbt_run folder. Inside here, there exist two main types of log files, the first created from the server machine, and the second created from individual worker runs.","title":"Where to find log files"},{"location":"log_files/#server-log-files","text":"Immediately inside the pbt_run folder are the log files created by the server. They contain many important information related to overall output of the run. Important log files include: best_worker.sofar , which lists the best worker until that point in the run. best_worker.done , which lists the best overall worker once the run is complete. decision_log.csv , which shows the result of binary tournament for all workers in all generations. For more information on binary tournament, refer to the binary tournament section. hp_log.csv , which shows the evolution of HPs across all workers and all generations.","title":"Server log files"},{"location":"log_files/#worker-log-files","text":"Inside the pbt_run folder, there exist many folders to reach the specific worker in a specific generation, which follow the format gX_WY , or Generation X, Worker Y. For instance, generation 3 worker 4 would have a full named g003_w04 . We can navigate inside these specific workers folders to look at a variety of log files related to a specific worker's output. These files include: hp_history.csv , a history of the hyper-parameters for that worker (shows the HP evolution for the particular worker) gradnorms.csv , the norm of the gradient in each epoch in that generation, for the worker perf_history.csv , the performance history over all completed generations for that particular worker hyperparameters.txt , the hyperparameters for the worker for that generation fitlog.csv , the training and validation loss and various model checkpoint files","title":"Worker log files"},{"location":"log_files/#client-log-files","text":"Client log files log the output from each process, and any errors within the client will be recorded here. To access these, go to the client_log folder which is inside the run folder. This is available only when the run is over.","title":"Client log files"},{"location":"navigate/","text":"What's Inside This Tutorial This tutorial offers the following: A fundamental overview of AutoLFADS, including general theory and a basic overview of the its architecture. A step-by-step tutorial on running AutoLFADS using Google Cloud Platform, on either sample data or your own neuronal spike data Instructions on analyzing the rates obtained from AutoLFADS How to Navigate This Tutorial The Introduction section includes an overview, general theory of architecture, and a general theory of Google Cloud Platform. This section offers the information necessary to utilize AutoLFADS, although skims over many details. The First Time Setup section offers a step-by-step tutorial on using Google Cloud Platform to install the architecture of AutoLFADS on GCP. Users are advised to go here first if they are trying to run AutoLFADS for the first time. Estimated time: 45 Minutes . The Run AutoLFADS section offers a step-by-step tutorial on using Google Cloud Platform to begin an AutoLFADS run on your dataset after the architecture has been installed. Returning users who have already setup the architecture, or have done a previous run are advised to go here. Estimated time: 2 Hours . The Analysis section offers a step-by-step tutorial on analyzing the output of AutoLFADS after the user has completed a run. Estimated time: 1.5 hours . The FAQ answers the most common questions and covers most common mistakes. Furthermore, there is an 'Additional Information' page at the end of each section, which details extra information or instructions which potentially are helpful for useres.","title":"Navigating the Tutorial"},{"location":"navigate/#whats-inside-this-tutorial","text":"This tutorial offers the following: A fundamental overview of AutoLFADS, including general theory and a basic overview of the its architecture. A step-by-step tutorial on running AutoLFADS using Google Cloud Platform, on either sample data or your own neuronal spike data Instructions on analyzing the rates obtained from AutoLFADS","title":"What's Inside This Tutorial"},{"location":"navigate/#how-to-navigate-this-tutorial","text":"The Introduction section includes an overview, general theory of architecture, and a general theory of Google Cloud Platform. This section offers the information necessary to utilize AutoLFADS, although skims over many details. The First Time Setup section offers a step-by-step tutorial on using Google Cloud Platform to install the architecture of AutoLFADS on GCP. Users are advised to go here first if they are trying to run AutoLFADS for the first time. Estimated time: 45 Minutes . The Run AutoLFADS section offers a step-by-step tutorial on using Google Cloud Platform to begin an AutoLFADS run on your dataset after the architecture has been installed. Returning users who have already setup the architecture, or have done a previous run are advised to go here. Estimated time: 2 Hours . The Analysis section offers a step-by-step tutorial on analyzing the output of AutoLFADS after the user has completed a run. Estimated time: 1.5 hours . The FAQ answers the most common questions and covers most common mistakes. Furthermore, there is an 'Additional Information' page at the end of each section, which details extra information or instructions which potentially are helpful for useres.","title":"How to Navigate This Tutorial"},{"location":"parameters/","text":"Shell Scripts Name Description server_name Name of the server VM. Used in the server_set_up.sh script zone Zone in which the VM will be created. Used for both - server and the client VMs. Used in the server_set_up.sh and machine_setup.sh scripts client_name The root name of the client VM created when you call machine_setup.sh script. When you create 3 client VMs with client_name set to \"clvm\", the client machines are named clvm1, clvm2 clvm3. Note that this is different from the tag name. Used in the machine_setup.sh script number_of_clients Number of client machines created when you call the machine_setup.sh script. Each client VM has a single GPU name_of_GPU The GPU used with each client machine. The default GPU is nvidia-tesla-k80 tag name This is the common alias of all client VMs. Helps in cases when a given command is needed to be run on all client VMs. We have set the tagname to \"pbtclient\" and this need not be changed Name Description PBT Script num_workers Number of workers in the population steps_to_ready Number of training steps until member of population pauses training and is ready to exploit and explore bucket_name Name of the cloud bucket data_path data directory inside the cloud bucket run_path Run directory inside the cloud bucket nprocess_gpu Number of processes to be run on each GPU Server Object Name Description epochs_per_generation Number of epochs per each generation max_generations Maximum number of generations. It may actually take lesser generations to converge to the best model, depending on the converging criterion defined in 'num_no_best_to_stop' and 'min_change_to_stop' explore_method The method used to explore hyper-parameters. Accepts two possible arguments - 'perturb' and 'resample' explore_param The parameters for the explore method defined by the 'explore_method' arg. When the 'explore_method' is set to 'perturb', the explore_param takes in a scalar between 0 and 1. A random number is then sampled uniformly from (1-explore_parama, 1+explore_param). This sampled number is then used as a factor which is multiplied to the HPs current value to perturb it. When the 'explore_method' is set to 'resample', the 'explore_param' must be set to None. In this condition, the new value of the HP is then obtained by resampling from a range of values defined for that parameter (Defined in the 'add_hp' method) mongo_server_ip The ip address of the VM on which mongo DB is running. By default, the mongo DB runs on the server and the ip address of the server is passed her port The port on which mongo DB runs (on the machine identified by mongo_server_ip) server_log_path The path where the server generated log files are stored. Not advised to change this parameter num_no_best_to_stop If no improvement is seen in the best worker for these many successive generations, PBT is terminated even before the max_generations are completed min_change_to_stop If the improvement in the best worker over successive generations is less than this quantity, the improvement is considered to be 0. Default value is 0.0005 (or .05 percentage ). If the num_no_best_to_stop is set to 5 and min_change_to_stop is set to 0.0005 , the PBT training will terminate if for 5 successive generations the improvement in the best worker is less than 0.05% over the previous best worker docker_name The name of the docker container. By default set to \"docker_pbt\" in the pbt_helper_fn.py Details For The Add_hp method Name Description name Name of the HP value Uses tuple to indicate range, or list/nparray for allowable values init_sample_mode 'Rand','logrand' or 'grid', mode of preferred initialization. Or pass explorable True or False, whether the 'explore' action should apply to this hyperparameter explore_method Same as the 'explore_method' defined for the Server class. The value of the 'explore_method' defined for the Server class is the default 'explore_method' for all HPs. However this value can be overwritten by passing it again for the specific HP. explore_param Same as the 'explore_param' defined for the Server class. This can be overwritten for a specific HP just like explore_method limit_explore If true, the new value of the HP after applying the explore method, cannot exceed the range defined by the \"value\" arg. LFADS Name Description keep_prob Dropout keep probability keep_ratio Coordinated dropout input keep probability l2_gen_scale L2 regularization cost for the generator l2_ic_enc_scale L2 regularization cost for the initial condition encoder l2_con_scale L2 cost for the controller l2_ci_enc_scale L2 cost for the controller input encoder kl_co_weight Strength of KL weight on controller output KL penalty kl_ic_weight Strength of KL weight on initial conditions KL penalty do_calc_r2 Calculate R^2 if the truth rates are available cell_clip_value Max value recurrent cell can take before being clipped prior_ar_atau Initial autocorrelation of AR(1) priors. This param is not searched often prior_ar_nvar Initial noise variance for AR(1) priors. This param is not searched often ckpt_save_interval Number of epochs between saving (non-lve) checkpoints. Doesn't has to be searched do_train_prior_ar_atau Determines whether the value for atau is trainable, or not. Boolean do_train_prior_ar_nvar Determines whether the value for noise variance is trainable, or not. Boolean kl_start_epoch Start increasing KL weight after these many epochs l2_start_epoch Start increasing l2 weight after these many epochs kl_increase_epochs Increase KL weight for these many epochs l2_increase_epochs Increase l2 weight for these many epochs batch_size Batch_size to use during training valid_batch_size Batch_size to use during validation val_cost_for_pbt Set to either held-out samples (\"heldout_samp\") or heldout trials (\"heldout_trial\"). Validation cost is computed over these cv_keep_ratio Cross-validation keep probability. Ratio of samples kept for training in the train set - if set to 80%, then 20% of samples from the training set are used for sample validation cd_grad_passthru_prob Probability of passing through gradients in coordinated dropout. Allows some percentage of gradients to backpropagate - if set to 0.1 , then 10% of gradients which were supposed to be blocked, are actually passed through factors_dim Number of factors from the generator ic_enc_dim Dimension of hidden state of the initial condition encoder ic_enc_seg_len Segment length passed to initial condition encoder for causal modeling. Set to 0 (default) gen_dim Size of hidden state for generator co_dim Dimensionality of the inferred inputs by the controller ci_enc_dim Size of the hidden state in the controller encoder con_dim \"Cell hidden size, controller\" - hidden state of the controller do_causal_controller Restrict the controller to infer only causal inputs. Boolean output_dist Spikes are modeled as observations of underlying rates, modeled as this distribution. Default - 'poisson' learning_rate_decay_factor Learning rate decay, decay by this fraction (How frequently is the decay applied) learning_rate_stop Stop training when the learning rate reaches this value learning_rate_n_to_compare The current cost has to be less than these many previous costs to lower learning rate checkpoint_pb_load_name Name of checkpoint files. Default - 'checkpoint' loss_scale Scaling of loss adam_epsilon Epsilon parameter for Adam optimizer beta1 Beta1 parameter for Adam optimizer beta2 Beta2 parameter for Adam optimizer data_filename_stem Prefix for the data filename (h5 file) data_dir Directory of the data h5 file do_train_readin Whether to train the read-in matrices and bias vectors. Boolean. False - leave them fixed at their initial values specified by the alignment matrices and vectors do_train_encoder_only Train only the encoder weights cv_rand_seed Random seed for held-out cross-validation sample mask output_filename_stem Name of output file (postfix will be added) max_ckpt_to_keep Max number of checkpoints to keep (keeps that many latest checkpoints) max_ckpt_to_keep_lve Max number of checkpoints to keep for lowest validation error models (keeps that many lowest validation error checkpoints) csv_log Name of file to keep the log of fit likelihoods (.csv appended to name) checkpoint_name Name of checkpoint files (.ckpt appended) device Which device to use (GPU/CPU). By default set to GPU. ps_nexamples_to_process Number of examples to process for posterior sample and average (not number of samples to average over) ic_prior_var Minimum variance of IC prior distribution ic_post_var_min Minimum variance of IC posterior distribution co_prior_var Variance of controller input prior distribution do_feed_factors_to_controller Should the controller network receive the feedback from the factors. Boolean. Should be set to True temporal_spike_jitter_width Jitters the spike, adds temporal noise during training. Avoids overfitting individual spikes inject_ext_input_to_gen Inject the external input to the generator (Boolean). Should be set to True allow_gpu_growth If true, only allocate the amount of memory needed for Session. Otherwise, use full GPU memory. Boolean max_grad_norm Maximum norm of gradient before gradient clipping is applied do_reset_learning_rate Reset the learning rate to initial value from the provided HP (HP - 'learning_rate_init'). Should be set to True","title":"Glossary of Parameters"},{"location":"pbtcode/","text":"This section works through important concepts arise specifically from the practical implementation of PBT in code. Exploit Called after the population has been trained a certain amount of steps ( steps_to_ready ), exploit involves lower performing models replacing its HPs and weights with that of a higher performing model. At the moment, exploit is only implemented in a single way, called 'Binary Tournament.' Binary Tournament In binary tournament, each member of the population randomly selects another member of the population, and copies its hyperparmaeters and weights if and only if the other member\u2019s performance is better. Whenever one member of the population is copied to another, all parameters \u2013 the hyperparameters, weights of the generator, and weights of the discriminator \u2013 are copied (from PBT paper). An animation of a 4 worker population using a binary tournament exploit method is shown. Explore Called if and only if a model has exploited the HPs and weights of a better performing, explore slightly alters the newly copied HPs. There are two choices for implementation of explore . Perturb Alters each HP value by a factor of 1.2 or 0.8. Resample Each hyperparameter is resampled from the original prior distribution defined with some probability. Processes vs Workers Ideally, PBT runs an entire population of workers in parallel. However, due to limits in computational power it might be only able to maximally run a batch of those workers at at time. For instance, a particular TPU might be capped at running, say, three workers at at time. Thus, a PBT run using a single one of these TPUs might want to run a population of 15 workers, but would only be able to train three at a time. In this case, the maximum numbers of workers would be trained until the they're ready to exploit and explore, in which case, they would \"wait\" and the next batch of workers would be trained up till that point until all the workers in the population are ready to exploit and explore. For instance, in the following diagram, a population of 6 workers are trained 3 processes at a time.","title":"Pbtcode"},{"location":"pbtcode/#exploit","text":"Called after the population has been trained a certain amount of steps ( steps_to_ready ), exploit involves lower performing models replacing its HPs and weights with that of a higher performing model. At the moment, exploit is only implemented in a single way, called 'Binary Tournament.'","title":"Exploit"},{"location":"pbtcode/#binary-tournament","text":"In binary tournament, each member of the population randomly selects another member of the population, and copies its hyperparmaeters and weights if and only if the other member\u2019s performance is better. Whenever one member of the population is copied to another, all parameters \u2013 the hyperparameters, weights of the generator, and weights of the discriminator \u2013 are copied (from PBT paper). An animation of a 4 worker population using a binary tournament exploit method is shown.","title":"Binary Tournament"},{"location":"pbtcode/#explore","text":"Called if and only if a model has exploited the HPs and weights of a better performing, explore slightly alters the newly copied HPs. There are two choices for implementation of explore .","title":"Explore"},{"location":"pbtcode/#perturb","text":"Alters each HP value by a factor of 1.2 or 0.8.","title":"Perturb"},{"location":"pbtcode/#resample","text":"Each hyperparameter is resampled from the original prior distribution defined with some probability.","title":"Resample"},{"location":"pbtcode/#processes-vs-workers","text":"Ideally, PBT runs an entire population of workers in parallel. However, due to limits in computational power it might be only able to maximally run a batch of those workers at at time. For instance, a particular TPU might be capped at running, say, three workers at at time. Thus, a PBT run using a single one of these TPUs might want to run a population of 15 workers, but would only be able to train three at a time. In this case, the maximum numbers of workers would be trained until the they're ready to exploit and explore, in which case, they would \"wait\" and the next batch of workers would be trained up till that point until all the workers in the population are ready to exploit and explore. For instance, in the following diagram, a population of 6 workers are trained 3 processes at a time.","title":"Processes vs Workers"},{"location":"runAddInfo/","text":"Modifying HPs While PBT is designed to automatically search for optimal hyperparameters during an AutoLFADS run, there are still adjustments we can make to the HPs prior to the run that can allow for better performance, such as a more optimal initialization value, or adjusting the ranges at which the HPs can vary. Go to the glossary for in-depth information on all parameters. Updating Docker image If you are re-using client machines created some time ago, there is a chance that the Docker image needs to be updated. The following steps explain how to update the Docker image on all machines. 1) First, make sure all client machines are started (have a green check next to them). 2) Open up Cloud Shell and then navigate to autoLFADS-beta/gclouds_scripts directory and run the following command. sh update_docker_image.sh 3) Next, wait for ~10 minutes for all Docker images to be successfully installed. This can be confirmed with the script sh check.sh pbtclient . Creating additional machines If you would like to add additional machines, you can use the same command when setting up the initial infrastructure . Simply navigate back to compute engine , open up 'Cloud Shell' in top right corner, and pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> . If creating additional client machines, we also need to re-add the user to Docker. To do so, first wait until Docker has finished successfully installing on the additional client machines (with check.sh ), and then run the following command in cloud shell. sh add_docker_user.sh pbtclient","title":"Additional Information"},{"location":"runAddInfo/#modifying-hps","text":"While PBT is designed to automatically search for optimal hyperparameters during an AutoLFADS run, there are still adjustments we can make to the HPs prior to the run that can allow for better performance, such as a more optimal initialization value, or adjusting the ranges at which the HPs can vary. Go to the glossary for in-depth information on all parameters.","title":"Modifying HPs"},{"location":"runAddInfo/#updating-docker-image","text":"If you are re-using client machines created some time ago, there is a chance that the Docker image needs to be updated. The following steps explain how to update the Docker image on all machines. 1) First, make sure all client machines are started (have a green check next to them). 2) Open up Cloud Shell and then navigate to autoLFADS-beta/gclouds_scripts directory and run the following command. sh update_docker_image.sh 3) Next, wait for ~10 minutes for all Docker images to be successfully installed. This can be confirmed with the script sh check.sh pbtclient .","title":"Updating Docker image"},{"location":"runAddInfo/#creating-additional-machines","text":"If you would like to add additional machines, you can use the same command when setting up the initial infrastructure . Simply navigate back to compute engine , open up 'Cloud Shell' in top right corner, and pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> . If creating additional client machines, we also need to re-add the user to Docker. To do so, first wait until Docker has finished successfully installing on the additional client machines (with check.sh ), and then run the following command in cloud shell. sh add_docker_user.sh pbtclient","title":"Creating additional machines"},{"location":"runLFADS/","text":"Overview Once, we have prepared our data, we now need to run some set-up scripts creating necessary VMs and TPU instances, and then run a python script that begins training. All these will take place on the Google Cloud Platform . Running Set-up Script First, navigate to the Google Cloud Compute Engine, located at https://console.cloud.google.com/compute . Then, click 'Activate Cloud Shell,' located in the toolbar to the right of the search. This should open up a black-background 'Cloud Shell' at the bottom of your Google Cloud Platform console. If this is the first you are using GCP, you can move on to the next section . However, if you have mlutiple Google Cloud projects, use the following line of code in the 'Cloud Shell' to set it to your project. gcloud config set project <your-project> Replace with the name of your project (omit < >) Create Bucket/TPUs/VMs Next, we will use a command that creates a bucket to hold the data, VMs for server and clients, and TPU instances. First, we want to add parameters to the following line of code. curl -s https://raw.githubusercontent.com/snel-repo/test-repo/master/create-vms.sh | bash /dev/stdin <server-name> <num-tpu> <bucket-name> <zone> <server-name> is the user's choice for the name of the VM server that is created. The server name can only be lower case, alphanumeric characters (so avoid underscores and other special characters.) The server name also has to be unique to the project (you can go to https://console.cloud.google.com/compute to check current server VM instances). <num-tpu> refers to how many TPU instances will be created for the run. <bucket-name> is the user's choice for the name of the bucket that is created. The bucket name can only be lower case, alphanumeric characters (so avoid capital letters, underscores and other special characters.) The bucket must also be unique, (you can go to https://console.cloud.google.com/storage to check current buckets) <zone> is a parameter that points towards the zone closest to the user. A list of available zones and their names is located here . Modify the following parameters (remove < > characters), separating each by a single space. For example, this is the line of code with parameters passed in. curl -s https://raw.githubusercontent.com/snel-repo/test-repo/master/create-vms.sh | bash /dev/stdin tutorialexamplevm 2 tutorialexamplebucket us-central1-f Then, copy and paste the command into the 'cloud shell' at the bottom of the Google Cloud Compute Engine, and then hit enter . The code will likely take several minutes to run. While the code is running, it should create a bucket to hold data, TPU instances corresponding to how many you chose, and a server VM under the name you chose. The code is finished running when you see your final TPU created, as in the image below. Once you see this, hit enter one more time. This should return control of the 'Cloud Shell.' Set-up Script Walkthrough Upload Data to Bucket Next, we want to upload our created .h5 file to our newly created bucket. First, navigate to the Cloud Storage section of GCP, located at https://console.cloud.google.com/storage Then, find the bucket you created in the browser section. Click on it, and then click on your 'data' directory. Drag your created .h5 file into that directory to upload it. Upload Walkthrough Start LFADS w/ PBT Run In your VM instances tab, find your VM-Server instance. Click on the button to the right of it labeled 'SSH' (you might have to scroll to the right) and then click 'Open in browser window.' Then, copy the pbt_script_1VM.py script to your home directory with the following command. cp /code/PBT_HP_opt/pbt_opt/pbt_script_1VM.py ~ Navigate to your home directory: cd ~ Once copied, we want to make some changes to the pbt_script_run_manager.py file such as editing parameters and setting paths. You can use whatever editor you would like, but in this tutorial we'll use nano. nano pbt_script_1VM.py Here, we want lines 21-24 to point towards the bucket we created. run_save_path = 'gs://pbt-test-bucket-2/runs/' # where PBT will store the runs output_folder = 'gs://pbt-test-bucket-2/output/' # where final PBT-LFADS output will be saved data_dir = 'gs://pbt-test-bucket-2/data' # data folder datafilename = 'lfads_datasset001.h5' Here, we want to simply edit the above in nano such that pbt-test-bucket-2 is replaced by the name of the bucket created. We also want to set datafilename to be the nam1e of the .h5 file we uploaded to bucket. For instance, if the bucket created is called tutorialexamplebucket and the .h5 file called data_for_lfads.h5 , then the block of code should be changed to: run_save_path = 'gs://tutorialexamplebucket/runs/' # where PBT will store the runs output_folder = 'gs://tutorialexamplebucket/output/' # where final PBT-LFADS output will be saved data_dir = 'gs://tutorialexamplebucket/data' # data folder datafilename = 'data_for_lfads.h5' If you're going through this tutorial using Lorenz sample data, then we can finish editing this python file here. However, if you're using your own data, you might want to change certain PBT parameters. Refer to the parameters section to see suggested parameters for your own data, as well as where to change them. To save changes and exit in nano, use the command ^x (ctrl-x), press Y , and then press enter. Then, we want to start a 'tmux' session, so we can run LFADS w/ PBT even when the terminal is closed. Enter the following command, with myname being anything you want to name your tmux session. tmux new -s myname Once a tmux session has been created, your terminal should have a green bar at the bottom with the tmux session name. In this tmux session, start your LFADS w/ PBT run with the following command. python pbt_script_1VM.py At this point, you have begun running LFADS w/ PBT. This will take some time: the sample data will likely take around 15 minutes using 2 TPUs, but larger datasets can take from hours to days. For more information on using tmux, such as detaching from the tmux terminal and killing a tmux session, refer to the tmux section. The run is finished when you see text in the tmux session similar to the following: ![](img/autoLFADS_final_screen.PNG] Run Script Walkthrough","title":"runLFADS"},{"location":"runLFADS/#overview","text":"Once, we have prepared our data, we now need to run some set-up scripts creating necessary VMs and TPU instances, and then run a python script that begins training. All these will take place on the Google Cloud Platform .","title":"Overview"},{"location":"runLFADS/#running-set-up-script","text":"First, navigate to the Google Cloud Compute Engine, located at https://console.cloud.google.com/compute . Then, click 'Activate Cloud Shell,' located in the toolbar to the right of the search. This should open up a black-background 'Cloud Shell' at the bottom of your Google Cloud Platform console. If this is the first you are using GCP, you can move on to the next section . However, if you have mlutiple Google Cloud projects, use the following line of code in the 'Cloud Shell' to set it to your project. gcloud config set project <your-project> Replace with the name of your project (omit < >)","title":"Running Set-up Script"},{"location":"runLFADS/#create-buckettpusvms","text":"Next, we will use a command that creates a bucket to hold the data, VMs for server and clients, and TPU instances. First, we want to add parameters to the following line of code. curl -s https://raw.githubusercontent.com/snel-repo/test-repo/master/create-vms.sh | bash /dev/stdin <server-name> <num-tpu> <bucket-name> <zone> <server-name> is the user's choice for the name of the VM server that is created. The server name can only be lower case, alphanumeric characters (so avoid underscores and other special characters.) The server name also has to be unique to the project (you can go to https://console.cloud.google.com/compute to check current server VM instances). <num-tpu> refers to how many TPU instances will be created for the run. <bucket-name> is the user's choice for the name of the bucket that is created. The bucket name can only be lower case, alphanumeric characters (so avoid capital letters, underscores and other special characters.) The bucket must also be unique, (you can go to https://console.cloud.google.com/storage to check current buckets) <zone> is a parameter that points towards the zone closest to the user. A list of available zones and their names is located here . Modify the following parameters (remove < > characters), separating each by a single space. For example, this is the line of code with parameters passed in. curl -s https://raw.githubusercontent.com/snel-repo/test-repo/master/create-vms.sh | bash /dev/stdin tutorialexamplevm 2 tutorialexamplebucket us-central1-f Then, copy and paste the command into the 'cloud shell' at the bottom of the Google Cloud Compute Engine, and then hit enter . The code will likely take several minutes to run. While the code is running, it should create a bucket to hold data, TPU instances corresponding to how many you chose, and a server VM under the name you chose. The code is finished running when you see your final TPU created, as in the image below. Once you see this, hit enter one more time. This should return control of the 'Cloud Shell.'","title":"Create Bucket/TPUs/VMs "},{"location":"runLFADS/#set-up-script-walkthrough","text":"","title":"Set-up Script Walkthrough"},{"location":"runLFADS/#upload-data-to-bucket","text":"Next, we want to upload our created .h5 file to our newly created bucket. First, navigate to the Cloud Storage section of GCP, located at https://console.cloud.google.com/storage Then, find the bucket you created in the browser section. Click on it, and then click on your 'data' directory. Drag your created .h5 file into that directory to upload it.","title":"Upload Data to Bucket "},{"location":"runLFADS/#upload-walkthrough","text":"","title":"Upload Walkthrough"},{"location":"runLFADS/#start-lfads-w-pbt-run","text":"In your VM instances tab, find your VM-Server instance. Click on the button to the right of it labeled 'SSH' (you might have to scroll to the right) and then click 'Open in browser window.' Then, copy the pbt_script_1VM.py script to your home directory with the following command. cp /code/PBT_HP_opt/pbt_opt/pbt_script_1VM.py ~ Navigate to your home directory: cd ~ Once copied, we want to make some changes to the pbt_script_run_manager.py file such as editing parameters and setting paths. You can use whatever editor you would like, but in this tutorial we'll use nano. nano pbt_script_1VM.py Here, we want lines 21-24 to point towards the bucket we created. run_save_path = 'gs://pbt-test-bucket-2/runs/' # where PBT will store the runs output_folder = 'gs://pbt-test-bucket-2/output/' # where final PBT-LFADS output will be saved data_dir = 'gs://pbt-test-bucket-2/data' # data folder datafilename = 'lfads_datasset001.h5' Here, we want to simply edit the above in nano such that pbt-test-bucket-2 is replaced by the name of the bucket created. We also want to set datafilename to be the nam1e of the .h5 file we uploaded to bucket. For instance, if the bucket created is called tutorialexamplebucket and the .h5 file called data_for_lfads.h5 , then the block of code should be changed to: run_save_path = 'gs://tutorialexamplebucket/runs/' # where PBT will store the runs output_folder = 'gs://tutorialexamplebucket/output/' # where final PBT-LFADS output will be saved data_dir = 'gs://tutorialexamplebucket/data' # data folder datafilename = 'data_for_lfads.h5' If you're going through this tutorial using Lorenz sample data, then we can finish editing this python file here. However, if you're using your own data, you might want to change certain PBT parameters. Refer to the parameters section to see suggested parameters for your own data, as well as where to change them. To save changes and exit in nano, use the command ^x (ctrl-x), press Y , and then press enter. Then, we want to start a 'tmux' session, so we can run LFADS w/ PBT even when the terminal is closed. Enter the following command, with myname being anything you want to name your tmux session. tmux new -s myname Once a tmux session has been created, your terminal should have a green bar at the bottom with the tmux session name. In this tmux session, start your LFADS w/ PBT run with the following command. python pbt_script_1VM.py At this point, you have begun running LFADS w/ PBT. This will take some time: the sample data will likely take around 15 minutes using 2 TPUs, but larger datasets can take from hours to days. For more information on using tmux, such as detaching from the tmux terminal and killing a tmux session, refer to the tmux section. The run is finished when you see text in the tmux session similar to the following: ![](img/autoLFADS_final_screen.PNG]","title":"Start LFADS w/ PBT Run "},{"location":"runLFADS/#run-script-walkthrough","text":"","title":"Run Script Walkthrough"},{"location":"run_autoLFADS/","text":"Now, we are ready to begin our run. All previous steps must be completed to this point. Warning Note that if you are doing an additional run, make sure that the folder you set your run_path to is completely empty. Also if you have a .zip file in your bucket named the same as your run_path , you must delete it before starting a new run. Beginning AutoLFADS in tmux First, make sure you're SSHed into your server VM. Then, make sure you're in the directory autoLFADS-beta/pbt_opt directory. If not, you can navigate with the following command: cd autoLFADS-beta/pbt_opt Then, we want to make sure to begin our run in a tmux terminal. This allows us to exit from the SSH terminal without terminating our run. First, type the following command into your SSH window (NOT cloud shell). tmux Then, making sure you're still in the autoLFADS-beta/pbt_opt directory, run the following command to begin your AutoLFADS run. python2 pbt_script_multiVM.py Your run should begin. If any errors pop-up, double check that the Docker images are finished pulling on the client machines with sh check.sh , and that mongoDB is running with service mongod status . If prompted for an SSH keygen, just hit enter to accept the default keygen. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section . Walkthrough Checking on your run At this point, you can do whatever with your local machine (including closing the Google Cloud Platform in your browser). In order to check back in on your run, first navigate to console.cloud.google.com/compute and then SSH back into your server. Once back in your server, we need to enter our tmux terminal. The command is: tmux a -t <tmux_session_name> tmux_session_name is by default 0 if you didn't specify a name. You can also check all your tmux sessions with the command tmux ls . To detach from your tmux terminal, the command is ctrl+b and then d .","title":"Run AutoLFADS"},{"location":"run_autoLFADS/#beginning-autolfads-in-tmux","text":"First, make sure you're SSHed into your server VM. Then, make sure you're in the directory autoLFADS-beta/pbt_opt directory. If not, you can navigate with the following command: cd autoLFADS-beta/pbt_opt Then, we want to make sure to begin our run in a tmux terminal. This allows us to exit from the SSH terminal without terminating our run. First, type the following command into your SSH window (NOT cloud shell). tmux Then, making sure you're still in the autoLFADS-beta/pbt_opt directory, run the following command to begin your AutoLFADS run. python2 pbt_script_multiVM.py Your run should begin. If any errors pop-up, double check that the Docker images are finished pulling on the client machines with sh check.sh , and that mongoDB is running with service mongod status . If prompted for an SSH keygen, just hit enter to accept the default keygen. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section .","title":"Beginning AutoLFADS in tmux "},{"location":"run_autoLFADS/#walkthrough","text":"","title":"Walkthrough"},{"location":"run_autoLFADS/#checking-on-your-run","text":"At this point, you can do whatever with your local machine (including closing the Google Cloud Platform in your browser). In order to check back in on your run, first navigate to console.cloud.google.com/compute and then SSH back into your server. Once back in your server, we need to enter our tmux terminal. The command is: tmux a -t <tmux_session_name> tmux_session_name is by default 0 if you didn't specify a name. You can also check all your tmux sessions with the command tmux ls . To detach from your tmux terminal, the command is ctrl+b and then d .","title":"Checking on your run"},{"location":"run_params/","text":"This step involves modifying the AutoLFADS python script to link to your VMs and bucket. Note that prior to this step, cloud infrastructure must be created already, the code must be cloned inside the server VM , the user must be added to docker group , and the bucket must be created and data uploaded to it. Linking AutoLFADS script to your data First, make sure you are SSHed back into your server VM. Then, (inside the SSH window), navigate to the autoLFADS-beta/pbt_opt directory with the following command. cd autoLFADS-beta/pbt_opt Next, we want to edit pbt_script_multiVM.py , the AutoLFADS run script, to link it to our data. Still inside the SSH window and inside the autoLFADS-beta/pbt_opt directory, open pbt_script_multiVM.py in a text editor of your choice. In this tutorial, we will do so in nano with the following command: nano pbt_script_multiVM.py A cheat sheet on using nano is available here . Now, you should see at the top something that looks like the following: First we need to set bucket_name, data_path, and run_path to point towards our bucket. Bucket_name refers to the name of the bucket we created, data_path to the folder where we hold our data, and run_path to the folder where our run output will be. Name can be set to any string we want our run to be called. In this tutorial, we'll modify these to be the following: bucket_name = 'autolfadsbucket' data_path = 'data' run_path = 'runs' name = 'tut_run' These changes to the script are necessary to link AutoLFADS to your data. However, there is also a variety of HPs in this script whose intial values, range of variance, and manner of pertubation by PBT can be edited. For information on explaining the various HPs and how to edit their initial values and ranges, go to the modifying HPs section in the Additional Information section. You can save your changes and exit your text editor.","title":"Set-up run parameters"},{"location":"run_params/#linking-autolfads-script-to-your-data","text":"First, make sure you are SSHed back into your server VM. Then, (inside the SSH window), navigate to the autoLFADS-beta/pbt_opt directory with the following command. cd autoLFADS-beta/pbt_opt Next, we want to edit pbt_script_multiVM.py , the AutoLFADS run script, to link it to our data. Still inside the SSH window and inside the autoLFADS-beta/pbt_opt directory, open pbt_script_multiVM.py in a text editor of your choice. In this tutorial, we will do so in nano with the following command: nano pbt_script_multiVM.py A cheat sheet on using nano is available here . Now, you should see at the top something that looks like the following: First we need to set bucket_name, data_path, and run_path to point towards our bucket. Bucket_name refers to the name of the bucket we created, data_path to the folder where we hold our data, and run_path to the folder where our run output will be. Name can be set to any string we want our run to be called. In this tutorial, we'll modify these to be the following: bucket_name = 'autolfadsbucket' data_path = 'data' run_path = 'runs' name = 'tut_run' These changes to the script are necessary to link AutoLFADS to your data. However, there is also a variety of HPs in this script whose intial values, range of variance, and manner of pertubation by PBT can be edited. For information on explaining the various HPs and how to edit their initial values and ranges, go to the modifying HPs section in the Additional Information section. You can save your changes and exit your text editor.","title":"Linking AutoLFADS script to your data "},{"location":"run_pbt/","text":"Running PBT The steps in this section are related to executing LFADS with PBT. Upload data to buckets The data uploaded to the bucket should look like this The h5 file should be prefixed with lfads . setting HPs and executing PBT To start the PBT run, you need to mount the shared storage and start docker containers on the clients. Only after this are we ready to start a PBT run. These two steps and starting PBT can be done from the pbt_script_multiVM.py on the server machine. You can also tweak the hyperparameter ranges in the same script - ~/pbt_opt/pbt_opt/pbt_script_multiVM.py You can edit the pbt script with a text editor of your choice. In the start of the script you need to provide the tag name for the client machines ( machine_name ), name of the shared storage bucket created previously ( bucket_name ), name of the docker container ( container_name - can be any alpha-numeric string), dir name for the data ( data_path ) and the path for saving the runs run_path Refer to the parameters section to know more about the params. After you have set the HPs in the script you can finally start PBT from the pbt_opt/pbt_opt dir as follows - cd pbt_opt/pbt_opt python pbt_script_multiVM.py This will start PBT on all machines and the server should show the following message -","title":"Running PBT"},{"location":"run_pbt/#running-pbt","text":"The steps in this section are related to executing LFADS with PBT.","title":"Running PBT"},{"location":"run_pbt/#upload-data-to-buckets","text":"The data uploaded to the bucket should look like this The h5 file should be prefixed with lfads .","title":"Upload data to buckets"},{"location":"run_pbt/#setting-hps-and-executing-pbt","text":"To start the PBT run, you need to mount the shared storage and start docker containers on the clients. Only after this are we ready to start a PBT run. These two steps and starting PBT can be done from the pbt_script_multiVM.py on the server machine. You can also tweak the hyperparameter ranges in the same script - ~/pbt_opt/pbt_opt/pbt_script_multiVM.py You can edit the pbt script with a text editor of your choice. In the start of the script you need to provide the tag name for the client machines ( machine_name ), name of the shared storage bucket created previously ( bucket_name ), name of the docker container ( container_name - can be any alpha-numeric string), dir name for the data ( data_path ) and the path for saving the runs run_path Refer to the parameters section to know more about the params. After you have set the HPs in the script you can finally start PBT from the pbt_opt/pbt_opt dir as follows - cd pbt_opt/pbt_opt python pbt_script_multiVM.py This will start PBT on all machines and the server should show the following message -","title":"setting HPs and executing PBT"},{"location":"setup/","text":"Overview The rest of this tutorial focuses on showing a step-by-step example run on training an LFADS model w/ PBT on Google Cloud Platform. This tutorial goes through the steps of how to prepare your data, train an LFADS model using TPUs on Google Cloud Platform, download the data back to your local computer, and then finally run a post-processing step on the data. In this example run we use sample Lorenz data, and you can follow along using either this same sample Lorenz data or by using your own neural data. However, note this tutorial is ONLY usable for binned neural spikes in the format [Neurons x Trial Length x Num Trials] in a .mat file. Download Tutorial Repo The first step is downloading the tutorial files onto your local computer, either by downloading the zip file here or using the commmand github clone github.com This tutorial contains utilities for preparing and post-processing your data, as well as the sample Lorenz data we will be running through LFADS. If you are using your own .mat neural data instead of the sample Lorenz data, copy your neural spiking data into the tutorial folder. Convert .mat to .h5 Tip Any section in this tutorial that contains a image has a corresponding video walkthrough. Preparing your data involves partitioning your data into testing and validation sets, and then converting that data from a .mat file to .h5 file. We use a simple matlab utility to accomplish this. Using Your Own Data If you are planning on going through this tutorial using sample Lorenz data, you can skip to the next section . Else, if you are planning on using your own data, first make sure your data is in the downloaded tutorial directory. Warning Your neural spiking data MUST be in the format Neurons x Trial Length x Num Trials in a .mat file Then, open the example_script.m file. We have to just make some slight modifications to point the script towards your data. (Again, these modifications are only necessary if you are using your only data and not sample Lorenz data). First, on line 2 , you should edit the line to read spikes = load('your_data.mat'); with your_data.mat being the path to your neural spiking data. You can then delete the entire line 3 . To be edited. Run .h5 Conversion Script Open the example_script.m in Matlab. Run the first two cells of the example_script. This should prepare the data, creating the .h5 file with validation and training sets. Prepare Data Walkthrough Accessing Google Cloud Platform Go to https://console.cloud.google.com/ and sign in with your Google account. If this is your first time logging into Google Cloud Platform, first we need to create a project. Navigate to the 'Compute Engine' and then fill out the details needed to create a project. Next, you have to link your account to a billing account by entering a credit card. Navigate back to the 'dashboard' and then click 'Billing,' located on the right side. Now we are ready to start a run. Info Pricing Details!! Pricing Details!! Free trial https://cloud.google.com/tpu/docs/pricing","title":"Setup"},{"location":"setup/#overview","text":"The rest of this tutorial focuses on showing a step-by-step example run on training an LFADS model w/ PBT on Google Cloud Platform. This tutorial goes through the steps of how to prepare your data, train an LFADS model using TPUs on Google Cloud Platform, download the data back to your local computer, and then finally run a post-processing step on the data. In this example run we use sample Lorenz data, and you can follow along using either this same sample Lorenz data or by using your own neural data. However, note this tutorial is ONLY usable for binned neural spikes in the format [Neurons x Trial Length x Num Trials] in a .mat file.","title":"Overview"},{"location":"setup/#download-tutorial-repo","text":"The first step is downloading the tutorial files onto your local computer, either by downloading the zip file here or using the commmand github clone github.com This tutorial contains utilities for preparing and post-processing your data, as well as the sample Lorenz data we will be running through LFADS. If you are using your own .mat neural data instead of the sample Lorenz data, copy your neural spiking data into the tutorial folder.","title":"Download Tutorial Repo"},{"location":"setup/#convert-mat-to-h5","text":"Tip Any section in this tutorial that contains a image has a corresponding video walkthrough. Preparing your data involves partitioning your data into testing and validation sets, and then converting that data from a .mat file to .h5 file. We use a simple matlab utility to accomplish this.","title":"Convert .mat to .h5 "},{"location":"setup/#using-your-own-data","text":"If you are planning on going through this tutorial using sample Lorenz data, you can skip to the next section . Else, if you are planning on using your own data, first make sure your data is in the downloaded tutorial directory. Warning Your neural spiking data MUST be in the format Neurons x Trial Length x Num Trials in a .mat file Then, open the example_script.m file. We have to just make some slight modifications to point the script towards your data. (Again, these modifications are only necessary if you are using your only data and not sample Lorenz data). First, on line 2 , you should edit the line to read spikes = load('your_data.mat'); with your_data.mat being the path to your neural spiking data. You can then delete the entire line 3 . To be edited.","title":"Using Your Own Data"},{"location":"setup/#run-h5-conversion-script","text":"Open the example_script.m in Matlab. Run the first two cells of the example_script. This should prepare the data, creating the .h5 file with validation and training sets.","title":"Run .h5 Conversion Script"},{"location":"setup/#prepare-data-walkthrough","text":"","title":"Prepare Data Walkthrough"},{"location":"setup/#accessing-google-cloud-platform","text":"Go to https://console.cloud.google.com/ and sign in with your Google account. If this is your first time logging into Google Cloud Platform, first we need to create a project. Navigate to the 'Compute Engine' and then fill out the details needed to create a project. Next, you have to link your account to a billing account by entering a credit card. Navigate back to the 'dashboard' and then click 'Billing,' located on the right side. Now we are ready to start a run. Info Pricing Details!! Pricing Details!! Free trial https://cloud.google.com/tpu/docs/pricing","title":"Accessing Google Cloud Platform"},{"location":"setupAddInfo/","text":"Stopping, Starting VMs Leaving VMs running can lead to unintended high bills. VMs can be stopped, deleted, and starting in the compute engine by clicking the checkbox to the left of the VM and clicking start/stop/delete buttons at the top of the list of VMs. Cloud Shell Many of the commands needed to set-up infrastructure are done using Google Cloud Shell. While the commands needed are listed directly in the tutorial, further information on navigating/using cloud shell can be found here: https://cloud.google.com/shell/docs/using-cloud-shell . Typical commands used for an AutoLFADS run are entering directories with cd , copying git repos with git clone , and running shell scripts with sh . Choosing number of clients In essence, each client VM can have 2-3 workers running on it, and the more total workers means a wider search for optimal hyperparameters. Thus, the more client VMs we create when setting up a run, the wider search for optimal hyperparameters. In general for most mid-sized datasets, having 24 workers usually will lead to finding optimal HPs. In this tutorial, we create 8 client VMs, which allows use of 24 workers. This number can be adjusted if the dataset is larger or smaller. Allocating Clients Between Multiple Zones If your AutoLFADS run involves a significant number of different client machines, but you lack the GPU quota in a single region, you can allocate these over a number of regions. For instance, if you want to create 8 clients machines, but only have a regional quota of 4 GPUs in either region, then you can create 4 in us-central1-c and 4 in us-east1-c by running the following commands consecutively. sh machine_setup.sh clientc 4 us-central1-c and then sh machine_setup.sh cliente 4 us-east1-c Requesting additional GPU quota Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, 24-48 hours before doing a run, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater).","title":"Additional Information"},{"location":"setupAddInfo/#stopping-starting-vms","text":"Leaving VMs running can lead to unintended high bills. VMs can be stopped, deleted, and starting in the compute engine by clicking the checkbox to the left of the VM and clicking start/stop/delete buttons at the top of the list of VMs.","title":"Stopping, Starting VMs"},{"location":"setupAddInfo/#cloud-shell","text":"Many of the commands needed to set-up infrastructure are done using Google Cloud Shell. While the commands needed are listed directly in the tutorial, further information on navigating/using cloud shell can be found here: https://cloud.google.com/shell/docs/using-cloud-shell . Typical commands used for an AutoLFADS run are entering directories with cd , copying git repos with git clone , and running shell scripts with sh .","title":"Cloud Shell"},{"location":"setupAddInfo/#choosing-number-of-clients","text":"In essence, each client VM can have 2-3 workers running on it, and the more total workers means a wider search for optimal hyperparameters. Thus, the more client VMs we create when setting up a run, the wider search for optimal hyperparameters. In general for most mid-sized datasets, having 24 workers usually will lead to finding optimal HPs. In this tutorial, we create 8 client VMs, which allows use of 24 workers. This number can be adjusted if the dataset is larger or smaller.","title":"Choosing number of clients"},{"location":"setupAddInfo/#allocating-clients-between-multiple-zones","text":"If your AutoLFADS run involves a significant number of different client machines, but you lack the GPU quota in a single region, you can allocate these over a number of regions. For instance, if you want to create 8 clients machines, but only have a regional quota of 4 GPUs in either region, then you can create 4 in us-central1-c and 4 in us-east1-c by running the following commands consecutively. sh machine_setup.sh clientc 4 us-central1-c and then sh machine_setup.sh cliente 4 us-east1-c","title":"Allocating Clients Between Multiple Zones"},{"location":"setupAddInfo/#requesting-additional-gpu-quota","text":"Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, 24-48 hours before doing a run, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater).","title":"Requesting additional GPU quota"},{"location":"setup_pbt/","text":"The steps in this section are essential to set-up the server and the client VMs for subsequent PBT runs. It has to be done only once - before the user tries PBT for the first time. All scripts/commands are run on the server. Make sure that all machines are started Get PBT code on the server machine In the home directory of the server VM, create a dir called PBT and get the PBT code in it. You can do this as follows - mkdir ~/code cd ~/code git clone -b cloud https://github.com/mrezak/pbt_opt.git Add the user to the docker group By default, the docker commands are to be prefaced with sudo . In order to avoid it, the user has to be added themselve to the docker group on all clients. The script for that is add_docker_user.sh in the cloud_gpu_scripts dir This step can be performed only when the client machines are ready and docker images are downloaded (was done in the background when we created the client machines). The complete process takes about 10-15 mins to run in the background. If installations have been complete, we should be able to see the docker image as follows - The script to add the user to the docker group on all client machines is add_docker_user.sh . It is present in the path ~/code/pbt_opt/cloud_gpu_scripts on the server. It is to be run as follows- sh add_docker_user.sh <root_name> <zone> The client virtual machines we had created earlier had the root name clientvm and were in the us-central1-c zone. To add the user in the docker group of all the client machines, the script is run as sh add_docker_user.sh clientvm us-central1-c The result of this operation is that the user can run docker commands without prefixing them with sudo , which is the default. You can read more about this here - Manage Docker as a non-root user After this step, we have created all infrastructure and configured it for the user","title":"Setup pbt"},{"location":"setup_pbt/#get-pbt-code-on-the-server-machine","text":"In the home directory of the server VM, create a dir called PBT and get the PBT code in it. You can do this as follows - mkdir ~/code cd ~/code git clone -b cloud https://github.com/mrezak/pbt_opt.git","title":"Get PBT code on the server machine"},{"location":"setup_pbt/#add-the-user-to-the-docker-group","text":"By default, the docker commands are to be prefaced with sudo . In order to avoid it, the user has to be added themselve to the docker group on all clients. The script for that is add_docker_user.sh in the cloud_gpu_scripts dir This step can be performed only when the client machines are ready and docker images are downloaded (was done in the background when we created the client machines). The complete process takes about 10-15 mins to run in the background. If installations have been complete, we should be able to see the docker image as follows - The script to add the user to the docker group on all client machines is add_docker_user.sh . It is present in the path ~/code/pbt_opt/cloud_gpu_scripts on the server. It is to be run as follows- sh add_docker_user.sh <root_name> <zone> The client virtual machines we had created earlier had the root name clientvm and were in the us-central1-c zone. To add the user in the docker group of all the client machines, the script is run as sh add_docker_user.sh clientvm us-central1-c The result of this operation is that the user can run docker commands without prefixing them with sudo , which is the default. You can read more about this here - Manage Docker as a non-root user After this step, we have created all infrastructure and configured it for the user","title":"Add the user to the docker group"},{"location":"theory/","text":"This section details a general theoretical overview of AutoLFADS. We can generally think of AutoLFADS as a combination of LFADS with a hyperparameter tuning framework called population-based training, or PBT. In order to understand AutoLFADS, we will first look at LFADS and PBT independently. What is LFADS? Populations of neurons show latent, low-dimensional structure that is not apparent from studying individual neurons, and this latent structure exhibits dynamics - a set of rules that determine how the system evolves. LFADS (Latent Factor Analysis of Dynamical Systems) is a deep learning tool which models these neural population dynamics. What is PBT? The ability of LFADS to learn and train from data hinges on many values set prior to training, called hyperparameters (HPs). We can think of HPs as essentially the 'settings' of LFADS --- with improper settings, LFADS can train slowly or incompletely. However, it is difficult to know what the \"correct\" settings are, as the ideal HPs differs from dataset to dataset! In this way, LFADS with unoptimized hyperparameters will sometimes struggle to fit correctly to data. PBT is a framework that allows optimization of hyperparameters. Thus, LFADS run with PBT, or what we deem AutoLFADS, can arrive at HPs that are far more optimized and can lead to far better performance than LFADS by itself. How does PBT work? PBT optimizes HPs during training through what is called 'evolutionary optimization,' a process modeled after evolutionary strategies. First, instead of training a single deep-learning model (for instance, LFADS) to completion, PBT generates an entire population of models, where each member of the population is an individual deep-learning model with randomly initialized unique HPs and weights. For convenience, an individual deep learning model with its own HPs and weights is deemed a 'worker.' With PBT, the entire population of workers is trained in parallel. During training, weaker performing workers (models with suboptimal HPs and thus suboptimal weights) copy the HPs and weights of better performing workers, in code called exploit . After exploiting, the values of the copied HPs are slightly changed as a way to search for more optimal HPs, in code called explore . In this way, instead of workers with poorly optimized HPs floundering for an entire run, they instead steal the weights and HPs of the best performing worker and then search from there for even better optimized HPs. Thus, by the end of all the workers training, we end up with a strong performing best worker which has highly optimized HPs. What is AutoLFADS? AutoLFADS is the training of LFADS with a PBT framework for HP optimization. While LFADS will struggle in being applied to less-structured behaviors or brain areas not primarily governed by intrinsic dynamics, AutoLFADS with its HP optimization can achieve good performance in these diverse datasets.","title":"Theory"},{"location":"theory/#what-is-lfads","text":"Populations of neurons show latent, low-dimensional structure that is not apparent from studying individual neurons, and this latent structure exhibits dynamics - a set of rules that determine how the system evolves. LFADS (Latent Factor Analysis of Dynamical Systems) is a deep learning tool which models these neural population dynamics.","title":"What is LFADS?"},{"location":"theory/#what-is-pbt","text":"The ability of LFADS to learn and train from data hinges on many values set prior to training, called hyperparameters (HPs). We can think of HPs as essentially the 'settings' of LFADS --- with improper settings, LFADS can train slowly or incompletely. However, it is difficult to know what the \"correct\" settings are, as the ideal HPs differs from dataset to dataset! In this way, LFADS with unoptimized hyperparameters will sometimes struggle to fit correctly to data. PBT is a framework that allows optimization of hyperparameters. Thus, LFADS run with PBT, or what we deem AutoLFADS, can arrive at HPs that are far more optimized and can lead to far better performance than LFADS by itself.","title":"What is PBT?"},{"location":"theory/#how-does-pbt-work","text":"PBT optimizes HPs during training through what is called 'evolutionary optimization,' a process modeled after evolutionary strategies. First, instead of training a single deep-learning model (for instance, LFADS) to completion, PBT generates an entire population of models, where each member of the population is an individual deep-learning model with randomly initialized unique HPs and weights. For convenience, an individual deep learning model with its own HPs and weights is deemed a 'worker.' With PBT, the entire population of workers is trained in parallel. During training, weaker performing workers (models with suboptimal HPs and thus suboptimal weights) copy the HPs and weights of better performing workers, in code called exploit . After exploiting, the values of the copied HPs are slightly changed as a way to search for more optimal HPs, in code called explore . In this way, instead of workers with poorly optimized HPs floundering for an entire run, they instead steal the weights and HPs of the best performing worker and then search from there for even better optimized HPs. Thus, by the end of all the workers training, we end up with a strong performing best worker which has highly optimized HPs.","title":"How does PBT work?"},{"location":"theory/#what-is-autolfads","text":"AutoLFADS is the training of LFADS with a PBT framework for HP optimization. While LFADS will struggle in being applied to less-structured behaviors or brain areas not primarily governed by intrinsic dynamics, AutoLFADS with its HP optimization can achieve good performance in these diverse datasets.","title":"What is AutoLFADS?"},{"location":"tmux/","text":"Tmux 'Tmux' is a terminal multiplexer allowing the user to access multiple separate terminal sessions inside a single terminal window. This allows us to run run LFADS w/ PBT without fear of accidentally closing the terminal and ending the run prematurely. Here are some important commands that allow use to navigate to and from the tmux terminal with the LFADS w/ PBT run. When you are in the 'tmux' terminal (green bar at the bottom), press ^b (ctrl-b) and then d to detach from the tmux terminal. When you have detached, you can reattach with the following line tmux a -t <name> with name being the tmux session name. When you are detached and want to instead look at the actual states of the clients, you can use the following command: tmux -L pbt_server a -t lf*0 . For a list of quick commands with tmux, use the tmux cheat sheet ...","title":"Tmux"},{"location":"tmux/#tmux","text":"'Tmux' is a terminal multiplexer allowing the user to access multiple separate terminal sessions inside a single terminal window. This allows us to run run LFADS w/ PBT without fear of accidentally closing the terminal and ending the run prematurely. Here are some important commands that allow use to navigate to and from the tmux terminal with the LFADS w/ PBT run. When you are in the 'tmux' terminal (green bar at the bottom), press ^b (ctrl-b) and then d to detach from the tmux terminal. When you have detached, you can reattach with the following line tmux a -t <name> with name being the tmux session name. When you are detached and want to instead look at the actual states of the clients, you can use the following command: tmux -L pbt_server a -t lf*0 . For a list of quick commands with tmux, use the tmux cheat sheet ...","title":"Tmux"},{"location":"visualize/","text":"While the run is going, here's how you visualize... * HP evolution * Other training graphs","title":"Visualize"}]}