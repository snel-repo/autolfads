{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoLFADS on Google Cloud Platform This tutorial is a programming-beginner friendly, step-by-step walkthrough on applying AutoLFADS, a deep learning tool that can be trained to uncover dynamics from single trial neural population data, using the computational resources of Google Cloud Platform. Quick introduction AutoLFADS is the combination of Latent Factor Analysis via Dynamical Systems (LFADS), a deep learning method to infer latent dynamics from single-trial neural population data, with Population Based Training (PBT), an automatic hyperparameter tuning framework. Specifically, this tutorial focuses on running AutoLFADS on Google Cloud Platform (GCP), which allows the use of AutoLFADS using computational resources available for rent on the cloud. Thus, as long as the user has access to GCP and the ability to pay for the usage of GPUs, then this tutorial can be used to apply AutoLFADS to neural population data without any need for local hardware. Requirements for this tutorial This tutorial is specifically designed for researchers and scientists with neural population data who are interested in using powerful deep-learning technology to uncover dynamics underlying neural populations. Fundamentally, this tutorial is designed so that significant programming knowledge or experience with deep-learning technology are NOT required. In order to use this tutorial, it is suggested that: Have neural population data in the format neurons x trial-length x number of trials , with sequence length <100 timesteps. Have access to Matlab and basic familiarity with it Have an email account associated with a university/institution and can pay for the usage of GPUs on the Google Cloud platform. For an estimated rate, a 2.5 hour AutoLFADS run with 8 GPUs (example run in tutorial) will cost ~$7. For more detailed pricing on GPUs, go to https://cloud.google.com/compute/gpus-pricing . Requesting GPU quota Google Cloud Platform enforces a GPU quota to prevent unforseen spikes in usage. Requesting additional quota must be done 24-48 hours in advance of a run. If interested in running AutoLFADS through GCP, it is recommended to first request additional GPU quota. Instructions are detailed in the First Time Set-up section.","title":"Is This Tutorial For You?"},{"location":"#autolfads-on-google-cloud-platform","text":"This tutorial is a programming-beginner friendly, step-by-step walkthrough on applying AutoLFADS, a deep learning tool that can be trained to uncover dynamics from single trial neural population data, using the computational resources of Google Cloud Platform.","title":"AutoLFADS on Google Cloud Platform"},{"location":"#quick-introduction","text":"AutoLFADS is the combination of Latent Factor Analysis via Dynamical Systems (LFADS), a deep learning method to infer latent dynamics from single-trial neural population data, with Population Based Training (PBT), an automatic hyperparameter tuning framework. Specifically, this tutorial focuses on running AutoLFADS on Google Cloud Platform (GCP), which allows the use of AutoLFADS using computational resources available for rent on the cloud. Thus, as long as the user has access to GCP and the ability to pay for the usage of GPUs, then this tutorial can be used to apply AutoLFADS to neural population data without any need for local hardware.","title":"Quick introduction"},{"location":"#requirements-for-this-tutorial","text":"This tutorial is specifically designed for researchers and scientists with neural population data who are interested in using powerful deep-learning technology to uncover dynamics underlying neural populations. Fundamentally, this tutorial is designed so that significant programming knowledge or experience with deep-learning technology are NOT required. In order to use this tutorial, it is suggested that: Have neural population data in the format neurons x trial-length x number of trials , with sequence length <100 timesteps. Have access to Matlab and basic familiarity with it Have an email account associated with a university/institution and can pay for the usage of GPUs on the Google Cloud platform. For an estimated rate, a 2.5 hour AutoLFADS run with 8 GPUs (example run in tutorial) will cost ~$7. For more detailed pricing on GPUs, go to https://cloud.google.com/compute/gpus-pricing .","title":"Requirements for this tutorial"},{"location":"#requesting-gpu-quota","text":"Google Cloud Platform enforces a GPU quota to prevent unforseen spikes in usage. Requesting additional quota must be done 24-48 hours in advance of a run. If interested in running AutoLFADS through GCP, it is recommended to first request additional GPU quota. Instructions are detailed in the First Time Set-up section.","title":"Requesting GPU quota"},{"location":"add_user/","text":"In this step, we add users to a Docker group in order to avoid prefacing docker commands with sudo . This step needs to be performed once per individual Google Account using AutoLFADS, before they start a run. Adding user to docker group First, navigate back to the compute engine and make sure the 'cloud shell' is open (button in the top right corner). Again, this step only needs to be done ONCE per Google account. Thus, if prior to this your Google account has already been adding to the docker group, this step does not need to be repeated. Furthermore, this step can only be done once the client machines are completely finished installing (remember to run check.sh to confirm this.) Once the cloud shell is open, run the following command sh add_docker_user.sh pbtclient . For this tutorial, the command would be sh add_docker_user.sh pbtclient Warning Leaving VMs running when not in use can lead to unintended high bills. Go to additional information section to see how to shut off unused VMs. Adding user to docker group walkthrough Pull AutoLFADS code onto server VM Next, we want to directly enter the server VM and copy the AutoLFADS code. In order to enter the server VM, we need to SSH in. First, make sure you're at the compute engine ( console.cloud.google.com/compute ) and can see your list of created VMs. Once you find your server you created (in this tutorial, its named tutserver), click on the button to the right of it labeled 'SSH'. You have now SSHed into your server client. Now, inside this newly opened SSH browser window (NOT the cloud shell), we want to clone the repository here in order to obtain the AutoLFADS python script. Run the following command in the SSH window. git clone -b GCP https://github.com/snel-repo/autolfads.git Now, the server VM should contain the necessary code. This completes creating all the necessary cloud infrastructure. Server Pull Code Walkthrough","title":"Set-up user"},{"location":"add_user/#adding-user-to-docker-group","text":"First, navigate back to the compute engine and make sure the 'cloud shell' is open (button in the top right corner). Again, this step only needs to be done ONCE per Google account. Thus, if prior to this your Google account has already been adding to the docker group, this step does not need to be repeated. Furthermore, this step can only be done once the client machines are completely finished installing (remember to run check.sh to confirm this.) Once the cloud shell is open, run the following command sh add_docker_user.sh pbtclient . For this tutorial, the command would be sh add_docker_user.sh pbtclient Warning Leaving VMs running when not in use can lead to unintended high bills. Go to additional information section to see how to shut off unused VMs.","title":"Adding user to docker group "},{"location":"add_user/#adding-user-to-docker-group-walkthrough","text":"","title":"Adding user to docker group walkthrough"},{"location":"add_user/#pull-autolfads-code-onto-server-vm","text":"Next, we want to directly enter the server VM and copy the AutoLFADS code. In order to enter the server VM, we need to SSH in. First, make sure you're at the compute engine ( console.cloud.google.com/compute ) and can see your list of created VMs. Once you find your server you created (in this tutorial, its named tutserver), click on the button to the right of it labeled 'SSH'. You have now SSHed into your server client. Now, inside this newly opened SSH browser window (NOT the cloud shell), we want to clone the repository here in order to obtain the AutoLFADS python script. Run the following command in the SSH window. git clone -b GCP https://github.com/snel-repo/autolfads.git Now, the server VM should contain the necessary code. This completes creating all the necessary cloud infrastructure.","title":"Pull AutoLFADS code onto server VM"},{"location":"add_user/#server-pull-code-walkthrough","text":"","title":"Server Pull Code Walkthrough"},{"location":"analysis/","text":"Info Estimated time for section: 30 min The following section details downloading the results from Google Cloud, and then running several analysis scripts. Downloading Data From GCP Once LFADS w/ PBT has finished running, we can now download the data back to our local computer for analysis. Warning At this point, you can stop all your machines (server and clients). Information on stopping machines can be found here To download data from the bucket, navigate back to https://console.cloud.google.com/storage . Click on the newly created zip file, and then click 'Download.' The output of the run should now be downloaded to your local computer. Post Processing Now that you have downloaded the data back to your local computer, open up the tutorial package ( download here ) Open up pbt_plot.m in Matlab. First, set the tutorial_package to your current working folder. Then, inside the pbt_plot.m script, we need to set the data_folder variable to the location of your pbt_run folder, which is located inside the run folder you downloaded. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\pbt_run . Set the output_folder variable to the folder where you want the plots generated. Run the pbt_plot.m script. This script will show the evolution of HPs over successive generations. These are some of the plots we got from the tutorial run. Compare to true rates Note This section is only for users using a synthetic dataset (as in the tutorial_package) where ground truth is available. If you used the synthetic dataset derived from Lorenz, then we can compare to the true rates. First, open the compare_rates.m script in the tutorial package. Fill in the first lfads_output_dir with the address of the lfads_output which is located inside your run folder. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\lfads_output . Then, you can run the script, which will generate R^2 value, which represents the error in the inferred rates compared to the true rates, as well as plot the inferred rates of several example neurons against their true underlying rates. The following plot was generated from this tutorial's run.","title":"Analysis"},{"location":"analysis/#downloading-data-from-gcp","text":"Once LFADS w/ PBT has finished running, we can now download the data back to our local computer for analysis. Warning At this point, you can stop all your machines (server and clients). Information on stopping machines can be found here To download data from the bucket, navigate back to https://console.cloud.google.com/storage . Click on the newly created zip file, and then click 'Download.' The output of the run should now be downloaded to your local computer.","title":"Downloading Data From GCP"},{"location":"analysis/#post-processing","text":"Now that you have downloaded the data back to your local computer, open up the tutorial package ( download here ) Open up pbt_plot.m in Matlab. First, set the tutorial_package to your current working folder. Then, inside the pbt_plot.m script, we need to set the data_folder variable to the location of your pbt_run folder, which is located inside the run folder you downloaded. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\pbt_run . Set the output_folder variable to the folder where you want the plots generated. Run the pbt_plot.m script. This script will show the evolution of HPs over successive generations. These are some of the plots we got from the tutorial run.","title":"Post Processing"},{"location":"analysis/#compare-to-true-rates","text":"Note This section is only for users using a synthetic dataset (as in the tutorial_package) where ground truth is available. If you used the synthetic dataset derived from Lorenz, then we can compare to the true rates. First, open the compare_rates.m script in the tutorial package. Fill in the first lfads_output_dir with the address of the lfads_output which is located inside your run folder. For instance, on my computer its C:\\\\Users\\tutorial\\output\\runs\\lfads_output . Then, you can run the script, which will generate R^2 value, which represents the error in the inferred rates compared to the true rates, as well as plot the inferred rates of several example neurons against their true underlying rates. The following plot was generated from this tutorial's run.","title":"Compare to true rates"},{"location":"architecture/","text":"Overview We will briefly describe the architecture of AutoLFADS, focusing on a high-level, theoretical overview over the basic organization and functionality of the various VMs created when beginning an AutoLFADS run. While this section describes the schema and theory behind the architecture of AutoLFADS, the steps to creating the basic architecture to begin an AutoLFADS runs are described beginning in first-time setup . Server and Client Server The architecture of AutoLFADS is largely centered on using several virtual machines (VMs), which we can imagine as emulations of real computers. In any AutoLFADS run, we first create a single server machine, which we can imagine as essentially 'overseeing' an AutoLFADS run. It triggers all the workers to begin training, oversees exploit/explore methodology, and receives various data back from the worker population. Client The next group of virtual machines we create are called 'client machines.' We can conceptualize the client machines as the VMs which actually run LFADS, which send and recieve information from the server machine. Each client machine has its own GPU and can run 3 processes at a time, in which each process can be used to train a worker. Using more client machines allows us to more quickly model our data when we want large numbers of workers. Docker, Shared Storage, and MongoDB While the interplay between server and client forms the backbone of an AutoLFADS run, there are various components that facilitate this. Docker The workers are run by client machines, but a LFADS worker generally requires a complex software setup. To avoid manual user set-up, each client machine has a Docker image with the required LFADS environment. For each client machine, each worker is run inside a Docker container. For more information on Docker, https://docs.docker.com/engine/docker-overview/ offers a more comprehensive overview. Shared Storage Shared storage contains the initial dataset AutoLFADS is analyzing, and throughout the run it stores the weights and HPs of each generation of worker. MongoDB We can imagine Mongo as facilitating the communication between server, clients, and shared storage. For instance, worker status is passed from the clients to server through Mongo.","title":"Architecture"},{"location":"architecture/#overview","text":"We will briefly describe the architecture of AutoLFADS, focusing on a high-level, theoretical overview over the basic organization and functionality of the various VMs created when beginning an AutoLFADS run. While this section describes the schema and theory behind the architecture of AutoLFADS, the steps to creating the basic architecture to begin an AutoLFADS runs are described beginning in first-time setup .","title":"Overview"},{"location":"architecture/#server-and-client","text":"","title":"Server and Client"},{"location":"architecture/#server","text":"The architecture of AutoLFADS is largely centered on using several virtual machines (VMs), which we can imagine as emulations of real computers. In any AutoLFADS run, we first create a single server machine, which we can imagine as essentially 'overseeing' an AutoLFADS run. It triggers all the workers to begin training, oversees exploit/explore methodology, and receives various data back from the worker population.","title":"Server"},{"location":"architecture/#client","text":"The next group of virtual machines we create are called 'client machines.' We can conceptualize the client machines as the VMs which actually run LFADS, which send and recieve information from the server machine. Each client machine has its own GPU and can run 3 processes at a time, in which each process can be used to train a worker. Using more client machines allows us to more quickly model our data when we want large numbers of workers.","title":"Client"},{"location":"architecture/#docker-shared-storage-and-mongodb","text":"While the interplay between server and client forms the backbone of an AutoLFADS run, there are various components that facilitate this.","title":"Docker, Shared Storage, and MongoDB"},{"location":"architecture/#docker","text":"The workers are run by client machines, but a LFADS worker generally requires a complex software setup. To avoid manual user set-up, each client machine has a Docker image with the required LFADS environment. For each client machine, each worker is run inside a Docker container. For more information on Docker, https://docs.docker.com/engine/docker-overview/ offers a more comprehensive overview.","title":"Docker"},{"location":"architecture/#shared-storage","text":"Shared storage contains the initial dataset AutoLFADS is analyzing, and throughout the run it stores the weights and HPs of each generation of worker.","title":"Shared Storage"},{"location":"architecture/#mongodb","text":"We can imagine Mongo as facilitating the communication between server, clients, and shared storage. For instance, worker status is passed from the clients to server through Mongo.","title":"MongoDB"},{"location":"client_tmux/","text":"During a run, there is a variety of ways to check on the status of a run. SSHing into client VMs We can SSH into the client VMs in order to look at individual workers or look at client log files. This can be helpful as this is where possible error logs will be stored. We SSH into clients in the same way we SSH into servers. Simply navigate to the compute engine, find the client VM you wish to enter, and click 'SSH.' Enter docker container Once SSHed into a client machine, we can now enter the bash of the docker container, where we can view the individual workers running on tmux sessions or look at log files. This can be done by running the following command on the client machine. docker exec -it docker_pbt /bin/bash This command will make you enter the docker container running on the client VM. To view all tmux sessions, you can type the command. tmux -L pbt_server ls This lists out all the tmux sessions. You can view any of these tmux sessions by using the command. tmux -L pbt_server a -t <tmux_session_name> For more information on connecting to the Docker container in the client machines can be found at https://linuxize.com/post/how-to-connect-to-docker-container . Viewing log files Log files generated from each client machines can be useful to diagnose potential errors. These files are generated at the end of a run in your bucket located at console.cloud.google.com/storage . To view the log files, enter your run folder. There will be a client_logs folder which contains these log files. These files can also be viewed during the run by entering docker container and entering the temp folder with: cd /tmp Now with ls you can see the list of log files. When is run over? There are various conditions which would end a run, and in your server's tmux session, it should display the text seen in the following image:","title":"During run"},{"location":"client_tmux/#sshing-into-client-vms","text":"We can SSH into the client VMs in order to look at individual workers or look at client log files. This can be helpful as this is where possible error logs will be stored. We SSH into clients in the same way we SSH into servers. Simply navigate to the compute engine, find the client VM you wish to enter, and click 'SSH.'","title":"SSHing into client VMs"},{"location":"client_tmux/#enter-docker-container","text":"Once SSHed into a client machine, we can now enter the bash of the docker container, where we can view the individual workers running on tmux sessions or look at log files. This can be done by running the following command on the client machine. docker exec -it docker_pbt /bin/bash This command will make you enter the docker container running on the client VM. To view all tmux sessions, you can type the command. tmux -L pbt_server ls This lists out all the tmux sessions. You can view any of these tmux sessions by using the command. tmux -L pbt_server a -t <tmux_session_name> For more information on connecting to the Docker container in the client machines can be found at https://linuxize.com/post/how-to-connect-to-docker-container .","title":"Enter docker container"},{"location":"client_tmux/#viewing-log-files","text":"Log files generated from each client machines can be useful to diagnose potential errors. These files are generated at the end of a run in your bucket located at console.cloud.google.com/storage . To view the log files, enter your run folder. There will be a client_logs folder which contains these log files. These files can also be viewed during the run by entering docker container and entering the temp folder with: cd /tmp Now with ls you can see the list of log files.","title":"Viewing log files"},{"location":"client_tmux/#when-is-run-over","text":"There are various conditions which would end a run, and in your server's tmux session, it should display the text seen in the following image:","title":"When is run over?"},{"location":"common_errors/","text":"Server Set-up error Several failures in the pipeline can be attributed to failure in server creation, which is done in the sh server_set_up.sh step. If the Python script fails to run, its likely worth first attempting to delete the server machine , and then re-run your server_set_up.sh script. After running the server_set_up.sh script, some of the last few lines of generated output should specify that the user has successfully been added to the MongoDB group, as seen in the below screenshot. If this isn't seen, likely MongoDB is failing to be set-up properly on the server machine. This can be checked by SSHing into the server machine and then running sudo service mongod status . Connection refused If you see a connection refused exit output after running the Python script, this might have to do with incorrect server set_up. While not exactly clear, it is likely worth checking that sh server_set_up.sh was successfully ran. Resource exhausted error If you see a resource exhausted error, usually following no response from process outputs, this might be due to having too large of a dataset. This tutorial recommends keeping the sequence length less than 100 timesteps.","title":"Common Errors"},{"location":"common_errors/#server-set-up-error","text":"Several failures in the pipeline can be attributed to failure in server creation, which is done in the sh server_set_up.sh step. If the Python script fails to run, its likely worth first attempting to delete the server machine , and then re-run your server_set_up.sh script. After running the server_set_up.sh script, some of the last few lines of generated output should specify that the user has successfully been added to the MongoDB group, as seen in the below screenshot. If this isn't seen, likely MongoDB is failing to be set-up properly on the server machine. This can be checked by SSHing into the server machine and then running sudo service mongod status .","title":"Server Set-up error"},{"location":"common_errors/#connection-refused","text":"If you see a connection refused exit output after running the Python script, this might have to do with incorrect server set_up. While not exactly clear, it is likely worth checking that sh server_set_up.sh was successfully ran.","title":"Connection refused"},{"location":"common_errors/#resource-exhausted-error","text":"If you see a resource exhausted error, usually following no response from process outputs, this might be due to having too large of a dataset. This tutorial recommends keeping the sequence length less than 100 timesteps.","title":"Resource exhausted error"},{"location":"create_bucket/","text":"Creating Bucket The next step is creating a bucket and adding two directories, one to upload your data to, and one to store checkpoints from your run. Navigate to console.cloud.google.com/storage and click 'Create Bucket.' The location type of the bucket should be \"multi-region,\" the storage class should be \"standard,\" and the access control should be \"fine-grained.\" After you have created your bucket, navigate inside your bucket by clicking on it. Inside your bucket, click \"create folder\" and name it \"data.\" Then create another folder called \"runs.\"","title":"Set-up storage infrastructure"},{"location":"create_bucket/#creating-bucket","text":"The next step is creating a bucket and adding two directories, one to upload your data to, and one to store checkpoints from your run. Navigate to console.cloud.google.com/storage and click 'Create Bucket.' The location type of the bucket should be \"multi-region,\" the storage class should be \"standard,\" and the access control should be \"fine-grained.\" After you have created your bucket, navigate inside your bucket by clicking on it. Inside your bucket, click \"create folder\" and name it \"data.\" Then create another folder called \"runs.\"","title":"Creating Bucket"},{"location":"create_infra/","text":"Info Estimated time for section: 45 minutes The following section details how to set-up cloud infrastructure on GCP. These steps need to be completed once per PROJECT. Set-up virtual machines Set-up storage infrastructure This step needs to be completed once per USER. That is, if a team is working on a project, each individual member with a Google Account has to execute the steps listed below. Set-up user Tutorial package This tutorial has a package that contains several scripts and a synthetic dataset useful for this tutorial. Note that depending on your specific needs, not every file in this package needs to be used. Download the tutorial package here . Create and configure the google cloud project This first step creates your project in Google Cloud Platform. This step only needs to be done once. Go to https://cloud.google.com/ and sign in with an account associated with your institution or university. Note that if you just sign in with your personal Google account, there is a chance that an eventual request for GPU quota will be denied, so it is highly suggested attempting to sign in with an institutional email address. If this is your first time logging into Google Cloud Platform, first we need to create a project. Navigate to the console.cloud.google.com/compute and then fill out the details needed to create a project. Next, you have to link your account to a billing account by entering a credit card. Navigate back to the dashboard at console.cloud.google.com/home and then click 'Billing,' located on the right side. Here you can add a method to pay for GPUs. Requesting additional GPU quota Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Google may take up to 48 hours to allocate quota (although generally much faster than this), so it is recommended to request additional quota well before planning to begin a run. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). On certain GCP accounts, this global GPU metric may not show up and if so, this specific quota can be ignored. Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater). On certain GCP accounts, this global CPU metric may not show up and if so, this specific quota can be ignored. Creating Server VM Once the project is created, we must create a server VM. To do so, first navigate back to the compute engine at console.cloud.google.com/compute . Then, we want to open the cloud shell by clicking the 'Activate Cloud Shell' button in the top right. Inside the shell of the gcloud project, we want to clone the SNEL repository. This has the collection of scripts which we will be using to create and set-up the server and client VMs and also to start PBT. git clone -b GCP https://github.com/snel-repo/autolfads.git Now, we want to navigate to the gcloud_scripts directory inside this repository. We can type the following command into our cloud shell to navigate to this directory. cd autolfads/gcloud_scripts Next, we want to run the server_set_up.sh script, which has the following format: sh server_set_up.sh <server_name> <zone> Server_name is the name of the server we create, and can be any lowercase alphanumeric characters. Zone is the region where your server is stored and used; best is usually the zone closest to you (list of zones can be found here: https://cloud.google.com/compute/docs/regions-zones/#available ) In this tutorial, we'll create a server named 'tutserver' and in the zone 'us-central1-c'. We will thus execute the following command in the cloud shell. sh server_set_up.sh tutserver us-central1-c This step will take a couple minutes to run. You will know its complete when a server is seen in list of VMs and the compute engine shows the following. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section . Create Server VM Walkthrough If for some reason the server VM doesn't seem to have been correctly created, check the common errors section for debugging help. Create the client machines After creating the server machines, we next want to create the client machines. To do so, first make sure you have the cloud shell still opened and inside the directory autolfads/gcloud_scripts . To create the client machines, we want to pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> <name_of_GPU> Client name is any lowercase alphanumeric characters naming the client, number_of_clients is how many client machines are created, and zone is the location the clients will be stored in. In order to decide how many clients to be created for your specific dataset, refer to additional information . Remember that the number of client machines is limited by your GPU quota in a particular zone, that is, your max number of GPUs in a zone is the max number of client machines you can create there . The argument only needs to be passed in if you are using a GPU that is not the default NVIDIA K80 GPU, and is the full name of the GPU without in lowercase with spaces replaced with dashes (for instance, nvidia-tesla-v100 ). In this tutorial, we will create 8 clients dispersed over two zones, as not to not exceed quota limits. Since we are using the NVIDIA K80 GPU we omit the <name_of_GPU> argument. To create these client machines in two zones, we run the following two lines of code consecutively. sh machine_setup.sh tutclientc 4 us-central1-c Once control of cloud shell is returned back: sh machine_setup.sh tutcliente 4 us-east1-c Create the client machines walkthrough Check if Docker is successfully installed Creating the clinet machines will take several minutes to run, and then an additional 10 or so minutes for the client machines to finish pulling the Docker image. Note that the creation of the client machines being finished is NOT indicated by control of Cloud Shell being returned or by there simply being a green checkmark next to the VMs; you must wait until the Docker image has been pulled on each machine. In order to see if the client machines have finished pulling the Docker image, run the following command in cloud shell. sh check.sh pbtclient Note pbtclient is the tagname, that is, a shorthand name for all client VMs in all zones. Commands that involve all client VMs at the same time, such as in the AutoLFADS python script and in check.sh, use pbtclient as the common client name. If the client machines are completely finished installing, then every client machine should return a Docker Installed on <client_name> Once all the machines have Docker properly installed, this marks the end of creating necessary VMs.","title":"Set-up virtual machines"},{"location":"create_infra/#tutorial-package","text":"This tutorial has a package that contains several scripts and a synthetic dataset useful for this tutorial. Note that depending on your specific needs, not every file in this package needs to be used. Download the tutorial package here .","title":"Tutorial package"},{"location":"create_infra/#create-and-configure-the-google-cloud-project","text":"This first step creates your project in Google Cloud Platform. This step only needs to be done once. Go to https://cloud.google.com/ and sign in with an account associated with your institution or university. Note that if you just sign in with your personal Google account, there is a chance that an eventual request for GPU quota will be denied, so it is highly suggested attempting to sign in with an institutional email address. If this is your first time logging into Google Cloud Platform, first we need to create a project. Navigate to the console.cloud.google.com/compute and then fill out the details needed to create a project. Next, you have to link your account to a billing account by entering a credit card. Navigate back to the dashboard at console.cloud.google.com/home and then click 'Billing,' located on the right side. Here you can add a method to pay for GPUs.","title":"Create and configure the google cloud project"},{"location":"create_infra/#requesting-additional-gpu-quota","text":"Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Google may take up to 48 hours to allocate quota (although generally much faster than this), so it is recommended to request additional quota well before planning to begin a run. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). On certain GCP accounts, this global GPU metric may not show up and if so, this specific quota can be ignored. Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater). On certain GCP accounts, this global CPU metric may not show up and if so, this specific quota can be ignored.","title":"Requesting additional GPU quota"},{"location":"create_infra/#creating-server-vm","text":"Once the project is created, we must create a server VM. To do so, first navigate back to the compute engine at console.cloud.google.com/compute . Then, we want to open the cloud shell by clicking the 'Activate Cloud Shell' button in the top right. Inside the shell of the gcloud project, we want to clone the SNEL repository. This has the collection of scripts which we will be using to create and set-up the server and client VMs and also to start PBT. git clone -b GCP https://github.com/snel-repo/autolfads.git Now, we want to navigate to the gcloud_scripts directory inside this repository. We can type the following command into our cloud shell to navigate to this directory. cd autolfads/gcloud_scripts Next, we want to run the server_set_up.sh script, which has the following format: sh server_set_up.sh <server_name> <zone> Server_name is the name of the server we create, and can be any lowercase alphanumeric characters. Zone is the region where your server is stored and used; best is usually the zone closest to you (list of zones can be found here: https://cloud.google.com/compute/docs/regions-zones/#available ) In this tutorial, we'll create a server named 'tutserver' and in the zone 'us-central1-c'. We will thus execute the following command in the cloud shell. sh server_set_up.sh tutserver us-central1-c This step will take a couple minutes to run. You will know its complete when a server is seen in list of VMs and the compute engine shows the following. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section .","title":"Creating Server VM "},{"location":"create_infra/#create-server-vm-walkthrough","text":"If for some reason the server VM doesn't seem to have been correctly created, check the common errors section for debugging help.","title":"Create Server VM Walkthrough"},{"location":"create_infra/#create-the-client-machines","text":"After creating the server machines, we next want to create the client machines. To do so, first make sure you have the cloud shell still opened and inside the directory autolfads/gcloud_scripts . To create the client machines, we want to pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> <name_of_GPU> Client name is any lowercase alphanumeric characters naming the client, number_of_clients is how many client machines are created, and zone is the location the clients will be stored in. In order to decide how many clients to be created for your specific dataset, refer to additional information . Remember that the number of client machines is limited by your GPU quota in a particular zone, that is, your max number of GPUs in a zone is the max number of client machines you can create there . The argument only needs to be passed in if you are using a GPU that is not the default NVIDIA K80 GPU, and is the full name of the GPU without in lowercase with spaces replaced with dashes (for instance, nvidia-tesla-v100 ). In this tutorial, we will create 8 clients dispersed over two zones, as not to not exceed quota limits. Since we are using the NVIDIA K80 GPU we omit the <name_of_GPU> argument. To create these client machines in two zones, we run the following two lines of code consecutively. sh machine_setup.sh tutclientc 4 us-central1-c Once control of cloud shell is returned back: sh machine_setup.sh tutcliente 4 us-east1-c","title":"Create the client machines "},{"location":"create_infra/#create-the-client-machines-walkthrough","text":"","title":"Create the client machines walkthrough"},{"location":"create_infra/#check-if-docker-is-successfully-installed","text":"Creating the clinet machines will take several minutes to run, and then an additional 10 or so minutes for the client machines to finish pulling the Docker image. Note that the creation of the client machines being finished is NOT indicated by control of Cloud Shell being returned or by there simply being a green checkmark next to the VMs; you must wait until the Docker image has been pulled on each machine. In order to see if the client machines have finished pulling the Docker image, run the following command in cloud shell. sh check.sh pbtclient Note pbtclient is the tagname, that is, a shorthand name for all client VMs in all zones. Commands that involve all client VMs at the same time, such as in the AutoLFADS python script and in check.sh, use pbtclient as the common client name. If the client machines are completely finished installing, then every client machine should return a Docker Installed on <client_name> Once all the machines have Docker properly installed, this marks the end of creating necessary VMs.","title":"Check if Docker is successfully installed"},{"location":"data/","text":"Info Estimated time for section: 2 hours This section details the steps for connecting your dataset to AutoLFADS and then beginning a run. The following steps have to be done everytime you want to begin an AutoLFADS run with a new dataset. If you already have data uploaded to the bucket, and you have already linked your run parameters , then you can proceed straight to the run autoLFADS section. At this point in the tutorial, you have already set up your cloud infrastructure . If you would like to add additional client machines, the way to do this is explained in add additional client machines section in the 'Additional Information' section. Note, if you are re-using client machines you have created some time ago, there is a chance you might have to update your Docker image on all client machines. Instructions for doing so are in the update Docker image section. Setting up your data Sample dataset This tutorial has a sample synthetic dataset derived from a Lorenz system that can be used to set-up an AutoLFADS run. This synthetic dataset is ideal to test out AutoLFADS as it is intrinsically low-D, and the inferred rates generated by AutoLFADS can easily be compared to its true rates to give a sense of the effectiveness of AutoLFADS. The synthetic dataset is located in the tutorial package, called lfads_data.h5 . If you have not downloaded it yet, you can download it here . If you would like to use this dataset, you can skip the next section on using your own data, and proceed straight to the 'Uploading data' section in this page. Using your own neural population data If you would like to use your own neuronal population data, the data must be a .mat file with a .spike attribute which contains spiking data in the format neurons x trial-length x number of trials . Furthermore, we suggest using data with a sequence length less than 100 timesteps; while there is not a clear data size limit, larger datasets are more prone to failure when running the python script. Once you have your dadta in a .mat file, you can use the convert_h5.m in the script zip file to convert it to the necessary .h5 file format in order to upload it to Google Cloud Platform. Simply open up the script, point the data field toward your .mat file and edit the lfads_input_file to point toward where you want the .h5 file to be generated. Optionally, you can also edit the valid_set_ratio variable to use a different amount of the data for validation. Once you have set these variables, you can run the script and it should output the converted .h5 file. Uploading Data Next, navigate back to console.cloud.google.com/storage , click on the bucket you created , and then navigate to your \"data\" folder. Then, upload your .h5 file here either by clicking upload file or by dragging and dropping. The .h5 file must be renamed to have the prefix specified by the data_file_namestem parameter in the pbt_script_multiVM.py file located in autolfads/pbt_opt directory of the SNEL repo. The default prefix is lfads , thus, in this tutorial we'll upload our data named lfads_data.h5. Our data folder in our bucket should now look like this with the data uploaded.","title":"Upload data"},{"location":"data/#setting-up-your-data","text":"","title":"Setting up your data"},{"location":"data/#sample-dataset","text":"This tutorial has a sample synthetic dataset derived from a Lorenz system that can be used to set-up an AutoLFADS run. This synthetic dataset is ideal to test out AutoLFADS as it is intrinsically low-D, and the inferred rates generated by AutoLFADS can easily be compared to its true rates to give a sense of the effectiveness of AutoLFADS. The synthetic dataset is located in the tutorial package, called lfads_data.h5 . If you have not downloaded it yet, you can download it here . If you would like to use this dataset, you can skip the next section on using your own data, and proceed straight to the 'Uploading data' section in this page.","title":"Sample dataset"},{"location":"data/#using-your-own-neural-population-data","text":"If you would like to use your own neuronal population data, the data must be a .mat file with a .spike attribute which contains spiking data in the format neurons x trial-length x number of trials . Furthermore, we suggest using data with a sequence length less than 100 timesteps; while there is not a clear data size limit, larger datasets are more prone to failure when running the python script. Once you have your dadta in a .mat file, you can use the convert_h5.m in the script zip file to convert it to the necessary .h5 file format in order to upload it to Google Cloud Platform. Simply open up the script, point the data field toward your .mat file and edit the lfads_input_file to point toward where you want the .h5 file to be generated. Optionally, you can also edit the valid_set_ratio variable to use a different amount of the data for validation. Once you have set these variables, you can run the script and it should output the converted .h5 file.","title":"Using your own neural population data"},{"location":"data/#uploading-data","text":"Next, navigate back to console.cloud.google.com/storage , click on the bucket you created , and then navigate to your \"data\" folder. Then, upload your .h5 file here either by clicking upload file or by dragging and dropping. The .h5 file must be renamed to have the prefix specified by the data_file_namestem parameter in the pbt_script_multiVM.py file located in autolfads/pbt_opt directory of the SNEL repo. The default prefix is lfads , thus, in this tutorial we'll upload our data named lfads_data.h5. Our data folder in our bucket should now look like this with the data uploaded.","title":"Uploading Data"},{"location":"flowchart/","text":"Overview The following quick start guide offers a general pipeline for the steps of running AutoLFADS over Google Cloud Platform, and each step can be expanded to offer abbreviated instructions including code and links to detailed sections. Brand new users are recommended to use the more detailed versions of instructions, beginning with First Time Set-up . (First run begin here) STEP 1 - Set Up Project 10 Minutes 1.1: Create project, set up billing 1.2: Request additional quota (if needed) Up to 48 Hour Wait STEP 2 - Set-up Infrastructure 45 Minutes 2.1: Clone SNEL repository into cloud shell (Code in red border should be executed in cloud shell) git clone -b GCP https://github.com/snel-repo/autolfads.git 2.2: Create server machine sh server_set_up.sh tutserver us-central1-c Up to 10 Minute Wait 2.3: Create client machines sh machine_setup.sh tutcliente 4 us-east1-c sh machine_setup.sh tutclientc 4 us-central1-c Up to 20 Minute Wait 2.4: Check if Docker is finished installing sucessfully sh check.sh pbtclient 2.5: Create bucket with folders data and run (New users to already existing project begin here) STEP 3 - Add User 5 Minutes (This step can only be done after Docker is finished installing) 3.1: Add user to Docker group sh add_docker_user.sh pbtclient 3.2: Clone SNEL repository on server (Code with blue border should be executed in server shell) git clone -b GCP https://github.com/snel-repo/autolfads.git (Additional run, new dataset begin here) STEP 4 - Upload Data 5 Minutes 4.1: Upload data with prefix lfads (Additional run, same dataset begin here) STEP 5 - Start Run 10 Minutes 5.1: Link pbt_script_multiVM.py to your data and edit parameters 5.2: Begin AutoLFADS run in tmux tmux python2 pbt_script_multiVM.py Up to 2 Hour Wait STEP 6 - Analyze Output 30 Minutes (You should stop all machines now) 6.1: Download output zip file from bucket 6.2: Run pbt_plot.m","title":"Quick Start"},{"location":"flowchart/#overview","text":"The following quick start guide offers a general pipeline for the steps of running AutoLFADS over Google Cloud Platform, and each step can be expanded to offer abbreviated instructions including code and links to detailed sections. Brand new users are recommended to use the more detailed versions of instructions, beginning with First Time Set-up . (First run begin here) STEP 1 - Set Up Project 10 Minutes 1.1: Create project, set up billing 1.2: Request additional quota (if needed) Up to 48 Hour Wait STEP 2 - Set-up Infrastructure 45 Minutes 2.1: Clone SNEL repository into cloud shell (Code in red border should be executed in cloud shell) git clone -b GCP https://github.com/snel-repo/autolfads.git 2.2: Create server machine sh server_set_up.sh tutserver us-central1-c Up to 10 Minute Wait 2.3: Create client machines sh machine_setup.sh tutcliente 4 us-east1-c sh machine_setup.sh tutclientc 4 us-central1-c Up to 20 Minute Wait 2.4: Check if Docker is finished installing sucessfully sh check.sh pbtclient 2.5: Create bucket with folders data and run (New users to already existing project begin here) STEP 3 - Add User 5 Minutes (This step can only be done after Docker is finished installing) 3.1: Add user to Docker group sh add_docker_user.sh pbtclient 3.2: Clone SNEL repository on server (Code with blue border should be executed in server shell) git clone -b GCP https://github.com/snel-repo/autolfads.git (Additional run, new dataset begin here) STEP 4 - Upload Data 5 Minutes 4.1: Upload data with prefix lfads (Additional run, same dataset begin here) STEP 5 - Start Run 10 Minutes 5.1: Link pbt_script_multiVM.py to your data and edit parameters 5.2: Begin AutoLFADS run in tmux tmux python2 pbt_script_multiVM.py Up to 2 Hour Wait STEP 6 - Analyze Output 30 Minutes (You should stop all machines now) 6.1: Download output zip file from bucket 6.2: Run pbt_plot.m","title":"Overview"},{"location":"introductionAddInfo/","text":"Binary Tournament Binary tournament is the algorithm by which different workers are compared against each other during 'exploit' phases.","title":"Additional Info"},{"location":"introductionAddInfo/#binary-tournament","text":"Binary tournament is the algorithm by which different workers are compared against each other during 'exploit' phases.","title":"Binary Tournament"},{"location":"log_files/","text":"Log files available after a run contain a variety of information on the run, including hyperparameter evolution, best workers, etc. This section briefly explores how to find and use these files. Where to find log files Log files will show up in the bucket you created. Then, enter the runs folder, and navigate to the created pbt_run folder. Inside here, there exist two main types of log files, the first created from the server machine, and the second created from individual worker runs. Server log files Immediately inside the pbt_run folder are the log files created by the server. They contain many important information related to overall output of the run. Important log files include: best_worker.sofar , which lists the best worker until that point in the run. best_worker.done , which lists the best overall worker once the run is complete. decision_log.csv , which shows the result of binary tournament for all workers in all generations. For more information on binary tournament, refer to the binary tournament section. hp_log.csv , which shows the evolution of HPs across all workers and all generations. Worker log files Inside the pbt_run folder, there exist many folders to reach the specific worker in a specific generation, which follow the format gX_WY , or Generation X, Worker Y. For instance, generation 3 worker 4 would have a full named g003_w04 . We can navigate inside these specific workers folders to look at a variety of log files related to a specific worker's output. These files include: hp_history.csv , a history of the hyper-parameters for that worker (shows the HP evolution for the particular worker) gradnorms.csv , the norm of the gradient in each epoch in that generation, for the worker perf_history.csv , the performance history over all completed generations for that particular worker hyperparameters.txt , the hyperparameters for the worker for that generation fitlog.csv , the training and validation loss and various model checkpoint files Client log files Client log files log the output from each process, and any errors within the client will be recorded here. To access these, go to the client_log folder which is inside the run folder. This is available only when the run is over.","title":"Log Files"},{"location":"log_files/#where-to-find-log-files","text":"Log files will show up in the bucket you created. Then, enter the runs folder, and navigate to the created pbt_run folder. Inside here, there exist two main types of log files, the first created from the server machine, and the second created from individual worker runs.","title":"Where to find log files"},{"location":"log_files/#server-log-files","text":"Immediately inside the pbt_run folder are the log files created by the server. They contain many important information related to overall output of the run. Important log files include: best_worker.sofar , which lists the best worker until that point in the run. best_worker.done , which lists the best overall worker once the run is complete. decision_log.csv , which shows the result of binary tournament for all workers in all generations. For more information on binary tournament, refer to the binary tournament section. hp_log.csv , which shows the evolution of HPs across all workers and all generations.","title":"Server log files"},{"location":"log_files/#worker-log-files","text":"Inside the pbt_run folder, there exist many folders to reach the specific worker in a specific generation, which follow the format gX_WY , or Generation X, Worker Y. For instance, generation 3 worker 4 would have a full named g003_w04 . We can navigate inside these specific workers folders to look at a variety of log files related to a specific worker's output. These files include: hp_history.csv , a history of the hyper-parameters for that worker (shows the HP evolution for the particular worker) gradnorms.csv , the norm of the gradient in each epoch in that generation, for the worker perf_history.csv , the performance history over all completed generations for that particular worker hyperparameters.txt , the hyperparameters for the worker for that generation fitlog.csv , the training and validation loss and various model checkpoint files","title":"Worker log files"},{"location":"log_files/#client-log-files","text":"Client log files log the output from each process, and any errors within the client will be recorded here. To access these, go to the client_log folder which is inside the run folder. This is available only when the run is over.","title":"Client log files"},{"location":"navigate/","text":"What's Inside This Tutorial This tutorial offers the following: A fundamental overview of AutoLFADS, including general theory and a basic overview of the its architecture. A step-by-step tutorial on running AutoLFADS using Google Cloud Platform, on either sample data or your own neuronal spike data Instructions on analyzing the rates obtained from AutoLFADS How to Navigate This Tutorial The Introduction section includes an overview, general theory of architecture, and a general theory of Google Cloud Platform. This section offers the information necessary to utilize AutoLFADS, although skims over many details. The First Time Setup section offers a step-by-step tutorial on using Google Cloud Platform to install the architecture of AutoLFADS on GCP. Users are advised to go here first if they are trying to run AutoLFADS for the first time. Estimated time: 45 Minutes . The Run AutoLFADS section offers a step-by-step tutorial on using Google Cloud Platform to begin an AutoLFADS run on your dataset after the architecture has been installed. Returning users who have already setup the architecture, or have done a previous run are advised to go here. Estimated time: 2 Hours . The Analysis section offers a step-by-step tutorial on analyzing the output of AutoLFADS after the user has completed a run. Estimated time: 1.5 hours . The FAQ answers the most common questions and covers most common mistakes. Furthermore, there is an 'Additional Information' page at the end of each section, which details extra information or instructions which potentially are helpful for useres.","title":"Navigating the Tutorial"},{"location":"navigate/#whats-inside-this-tutorial","text":"This tutorial offers the following: A fundamental overview of AutoLFADS, including general theory and a basic overview of the its architecture. A step-by-step tutorial on running AutoLFADS using Google Cloud Platform, on either sample data or your own neuronal spike data Instructions on analyzing the rates obtained from AutoLFADS","title":"What's Inside This Tutorial"},{"location":"navigate/#how-to-navigate-this-tutorial","text":"The Introduction section includes an overview, general theory of architecture, and a general theory of Google Cloud Platform. This section offers the information necessary to utilize AutoLFADS, although skims over many details. The First Time Setup section offers a step-by-step tutorial on using Google Cloud Platform to install the architecture of AutoLFADS on GCP. Users are advised to go here first if they are trying to run AutoLFADS for the first time. Estimated time: 45 Minutes . The Run AutoLFADS section offers a step-by-step tutorial on using Google Cloud Platform to begin an AutoLFADS run on your dataset after the architecture has been installed. Returning users who have already setup the architecture, or have done a previous run are advised to go here. Estimated time: 2 Hours . The Analysis section offers a step-by-step tutorial on analyzing the output of AutoLFADS after the user has completed a run. Estimated time: 1.5 hours . The FAQ answers the most common questions and covers most common mistakes. Furthermore, there is an 'Additional Information' page at the end of each section, which details extra information or instructions which potentially are helpful for useres.","title":"How to Navigate This Tutorial"},{"location":"parameters/","text":"Shell Scripts Name Description server_name Name of the server VM. Used in the server_set_up.sh script zone Zone in which the VM will be created. Used for both - server and the client VMs. Used in the server_set_up.sh and machine_setup.sh scripts client_name The root name of the client VM created when you call machine_setup.sh script. When you create 3 client VMs with client_name set to \"clvm\", the client machines are named clvm1, clvm2 clvm3. Note that this is different from the tag name. Used in the machine_setup.sh script number_of_clients Number of client machines created when you call the machine_setup.sh script. Each client VM has a single GPU name_of_GPU The GPU used with each client machine. The default GPU is nvidia-tesla-k80 tag name This is the common alias of all client VMs. Helps in cases when a given command is needed to be run on all client VMs. We have set the tagname to \"pbtclient\" and this need not be changed Name Description PBT Script num_workers Number of workers in the population steps_to_ready Number of training steps until member of population pauses training and is ready to exploit and explore bucket_name Name of the cloud bucket data_path data directory inside the cloud bucket run_path Run directory inside the cloud bucket nprocess_gpu Number of processes to be run on each GPU Server Object Name Description epochs_per_generation Number of epochs per each generation max_generations Maximum number of generations. It may actually take lesser generations to converge to the best model, depending on the converging criterion defined in 'num_no_best_to_stop' and 'min_change_to_stop' explore_method The method used to explore hyper-parameters. Accepts two possible arguments - 'perturb' and 'resample' explore_param The parameters for the explore method defined by the 'explore_method' arg. When the 'explore_method' is set to 'perturb', the explore_param takes in a scalar between 0 and 1. A random number is then sampled uniformly from (1-explore_parama, 1+explore_param). This sampled number is then used as a factor which is multiplied to the HPs current value to perturb it. When the 'explore_method' is set to 'resample', the 'explore_param' must be set to None. In this condition, the new value of the HP is then obtained by resampling from a range of values defined for that parameter (Defined in the 'add_hp' method) mongo_server_ip The ip address of the VM on which mongo DB is running. By default, the mongo DB runs on the server and the ip address of the server is passed her port The port on which mongo DB runs (on the machine identified by mongo_server_ip) server_log_path The path where the server generated log files are stored. Not advised to change this parameter num_no_best_to_stop If no improvement is seen in the best worker for these many successive generations, PBT is terminated even before the max_generations are completed min_change_to_stop If the improvement in the best worker over successive generations is less than this quantity, the improvement is considered to be 0. Default value is 0.0005 (or .05 percentage ). If the num_no_best_to_stop is set to 5 and min_change_to_stop is set to 0.0005 , the PBT training will terminate if for 5 successive generations the improvement in the best worker is less than 0.05% over the previous best worker docker_name The name of the docker container. By default set to \"docker_pbt\" in the pbt_helper_fn.py Details For The Add_hp method Name Description name Name of the HP value Uses tuple to indicate range, or list/nparray for allowable values init_sample_mode 'Rand','logrand' or 'grid', mode of preferred initialization. Or pass explorable True or False, whether the 'explore' action should apply to this hyperparameter explore_method Same as the 'explore_method' defined for the Server class. The value of the 'explore_method' defined for the Server class is the default 'explore_method' for all HPs. However this value can be overwritten by passing it again for the specific HP. explore_param Same as the 'explore_param' defined for the Server class. This can be overwritten for a specific HP just like explore_method limit_explore If true, the new value of the HP after applying the explore method, cannot exceed the range defined by the \"value\" arg. LFADS Name Description keep_prob Dropout keep probability keep_ratio Coordinated dropout input keep probability l2_gen_scale L2 regularization cost for the generator l2_ic_enc_scale L2 regularization cost for the initial condition encoder l2_con_scale L2 cost for the controller l2_ci_enc_scale L2 cost for the controller input encoder kl_co_weight Strength of KL weight on controller output KL penalty kl_ic_weight Strength of KL weight on initial conditions KL penalty do_calc_r2 Calculate R^2 if the truth rates are available cell_clip_value Max value recurrent cell can take before being clipped prior_ar_atau Initial autocorrelation of AR(1) priors. This param is not searched often prior_ar_nvar Initial noise variance for AR(1) priors. This param is not searched often ckpt_save_interval Number of epochs between saving (non-lve) checkpoints. Doesn't has to be searched do_train_prior_ar_atau Determines whether the value for atau is trainable, or not. Boolean do_train_prior_ar_nvar Determines whether the value for noise variance is trainable, or not. Boolean kl_start_epoch Start increasing KL weight after these many epochs l2_start_epoch Start increasing l2 weight after these many epochs kl_increase_epochs Increase KL weight for these many epochs l2_increase_epochs Increase l2 weight for these many epochs batch_size Batch_size to use during training valid_batch_size Batch_size to use during validation val_cost_for_pbt Set to either held-out samples (\"heldout_samp\") or heldout trials (\"heldout_trial\"). Validation cost is computed over these cv_keep_ratio Cross-validation keep probability. Ratio of samples kept for training in the train set - if set to 80%, then 20% of samples from the training set are used for sample validation cd_grad_passthru_prob Probability of passing through gradients in coordinated dropout. Allows some percentage of gradients to backpropagate - if set to 0.1 , then 10% of gradients which were supposed to be blocked, are actually passed through factors_dim Number of factors from the generator ic_enc_dim Dimension of hidden state of the initial condition encoder ic_enc_seg_len Segment length passed to initial condition encoder for causal modeling. Set to 0 (default) gen_dim Size of hidden state for generator co_dim Dimensionality of the inferred inputs by the controller ci_enc_dim Size of the hidden state in the controller encoder con_dim \"Cell hidden size, controller\" - hidden state of the controller do_causal_controller Restrict the controller to infer only causal inputs. Boolean output_dist Spikes are modeled as observations of underlying rates, modeled as this distribution. Default - 'poisson' learning_rate_decay_factor Learning rate decay, decay by this fraction (How frequently is the decay applied) learning_rate_stop Stop training when the learning rate reaches this value learning_rate_n_to_compare The current cost has to be less than these many previous costs to lower learning rate checkpoint_pb_load_name Name of checkpoint files. Default - 'checkpoint' loss_scale Scaling of loss adam_epsilon Epsilon parameter for Adam optimizer beta1 Beta1 parameter for Adam optimizer beta2 Beta2 parameter for Adam optimizer data_filename_stem Prefix for the data filename (h5 file) data_dir Directory of the data h5 file do_train_readin Whether to train the read-in matrices and bias vectors. Boolean. False - leave them fixed at their initial values specified by the alignment matrices and vectors do_train_encoder_only Train only the encoder weights cv_rand_seed Random seed for held-out cross-validation sample mask output_filename_stem Name of output file (postfix will be added) max_ckpt_to_keep Max number of checkpoints to keep (keeps that many latest checkpoints) max_ckpt_to_keep_lve Max number of checkpoints to keep for lowest validation error models (keeps that many lowest validation error checkpoints) csv_log Name of file to keep the log of fit likelihoods (.csv appended to name) checkpoint_name Name of checkpoint files (.ckpt appended) device Which device to use (GPU/CPU). By default set to GPU. ps_nexamples_to_process Number of examples to process for posterior sample and average (not number of samples to average over) ic_prior_var Minimum variance of IC prior distribution ic_post_var_min Minimum variance of IC posterior distribution co_prior_var Variance of controller input prior distribution do_feed_factors_to_controller Should the controller network receive the feedback from the factors. Boolean. Should be set to True temporal_spike_jitter_width Jitters the spike, adds temporal noise during training. Avoids overfitting individual spikes inject_ext_input_to_gen Inject the external input to the generator (Boolean). Should be set to True allow_gpu_growth If true, only allocate the amount of memory needed for Session. Otherwise, use full GPU memory. Boolean max_grad_norm Maximum norm of gradient before gradient clipping is applied do_reset_learning_rate Reset the learning rate to initial value from the provided HP (HP - 'learning_rate_init'). Should be set to True","title":"Glossary of Parameters"},{"location":"runAddInfo/","text":"Modifying HPs While PBT is designed to automatically search for optimal hyperparameters during an AutoLFADS run, there are still adjustments we can make to the HPs prior to the run that can allow for better performance, such as a more optimal initialization value, or adjusting the ranges at which the HPs can vary. Go to the glossary for in-depth information on all parameters. Updating Docker image If you are re-using client machines created some time ago, there is a chance that the Docker image needs to be updated. The following steps explain how to update the Docker image on all machines. 1) First, make sure all client machines are started (have a green check next to them). 2) Open up Cloud Shell and then navigate to autolfads/gclouds_scripts directory and run the following command. sh update_docker_image.sh 3) Next, wait for ~10 minutes for all Docker images to be successfully installed. This can be confirmed with the script sh check.sh pbtclient . Creating additional machines If you would like to add additional machines, you can use the same command when setting up the initial infrastructure . Simply navigate back to compute engine , open up 'Cloud Shell' in top right corner, and pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> . If creating additional client machines, we also need to re-add the user to Docker. To do so, first wait until Docker has finished successfully installing on the additional client machines (with check.sh ), and then run the following command in cloud shell. sh add_docker_user.sh pbtclient","title":"Additional Information"},{"location":"runAddInfo/#modifying-hps","text":"While PBT is designed to automatically search for optimal hyperparameters during an AutoLFADS run, there are still adjustments we can make to the HPs prior to the run that can allow for better performance, such as a more optimal initialization value, or adjusting the ranges at which the HPs can vary. Go to the glossary for in-depth information on all parameters.","title":"Modifying HPs"},{"location":"runAddInfo/#updating-docker-image","text":"If you are re-using client machines created some time ago, there is a chance that the Docker image needs to be updated. The following steps explain how to update the Docker image on all machines. 1) First, make sure all client machines are started (have a green check next to them). 2) Open up Cloud Shell and then navigate to autolfads/gclouds_scripts directory and run the following command. sh update_docker_image.sh 3) Next, wait for ~10 minutes for all Docker images to be successfully installed. This can be confirmed with the script sh check.sh pbtclient .","title":"Updating Docker image"},{"location":"runAddInfo/#creating-additional-machines","text":"If you would like to add additional machines, you can use the same command when setting up the initial infrastructure . Simply navigate back to compute engine , open up 'Cloud Shell' in top right corner, and pass in parameters into the following command: sh machine_setup.sh <client_name> <number_of_clients> <zone> . If creating additional client machines, we also need to re-add the user to Docker. To do so, first wait until Docker has finished successfully installing on the additional client machines (with check.sh ), and then run the following command in cloud shell. sh add_docker_user.sh pbtclient","title":"Creating additional machines"},{"location":"run_autoLFADS/","text":"Now, we are ready to begin our run. All previous steps must be completed to this point. Warning Note that if you are doing an additional run, make sure that the folder you set your run_path to is completely empty. Also if you have a .zip file in your bucket named the same as your run_path , you must delete it before starting a new run. Beginning AutoLFADS in tmux First, make sure you're SSHed into your server VM. Then, make sure you're in the directory autolfads/pbt_opt directory. If not, you can navigate with the following command: cd autolfads/pbt_opt Then, we want to make sure to begin our run in a tmux terminal. This allows us to exit from the SSH terminal without terminating our run. First, type the following command into your SSH window (NOT cloud shell). tmux Then, making sure you're still in the autolfads/pbt_opt directory, run the following command to begin your AutoLFADS run. python2 pbt_script_multiVM.py Your run should begin. If any errors pop-up, double check that the Docker images are finished pulling on the client machines with sh check.sh , and that mongoDB is running with service mongod status . If prompted for an SSH keygen, just hit enter to accept the default keygen. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section . Walkthrough Checking on your run At this point, you can do whatever with your local machine (including closing the Google Cloud Platform in your browser). In order to check back in on your run, first navigate to console.cloud.google.com/compute and then SSH back into your server. Once back in your server, we need to enter our tmux terminal. The command is: tmux a -t <tmux_session_name> tmux_session_name is by default 0 if you didn't specify a name. You can also check all your tmux sessions with the command tmux ls . To detach from your tmux terminal, the command is ctrl+b and then d .","title":"Run AutoLFADS"},{"location":"run_autoLFADS/#beginning-autolfads-in-tmux","text":"First, make sure you're SSHed into your server VM. Then, make sure you're in the directory autolfads/pbt_opt directory. If not, you can navigate with the following command: cd autolfads/pbt_opt Then, we want to make sure to begin our run in a tmux terminal. This allows us to exit from the SSH terminal without terminating our run. First, type the following command into your SSH window (NOT cloud shell). tmux Then, making sure you're still in the autolfads/pbt_opt directory, run the following command to begin your AutoLFADS run. python2 pbt_script_multiVM.py Your run should begin. If any errors pop-up, double check that the Docker images are finished pulling on the client machines with sh check.sh , and that mongoDB is running with service mongod status . If prompted for an SSH keygen, just hit enter to accept the default keygen. Warning Active VMs can rack up costs when left unattended. Remember to shut them down when not in use. For information on how to stop, pause, and start VMs, go to this section .","title":"Beginning AutoLFADS in tmux "},{"location":"run_autoLFADS/#walkthrough","text":"","title":"Walkthrough"},{"location":"run_autoLFADS/#checking-on-your-run","text":"At this point, you can do whatever with your local machine (including closing the Google Cloud Platform in your browser). In order to check back in on your run, first navigate to console.cloud.google.com/compute and then SSH back into your server. Once back in your server, we need to enter our tmux terminal. The command is: tmux a -t <tmux_session_name> tmux_session_name is by default 0 if you didn't specify a name. You can also check all your tmux sessions with the command tmux ls . To detach from your tmux terminal, the command is ctrl+b and then d .","title":"Checking on your run"},{"location":"run_params/","text":"This step involves modifying the AutoLFADS python script to link to your VMs and bucket. Note that prior to this step, cloud infrastructure must be created already, the code must be cloned inside the server VM , the user must be added to docker group , and the bucket must be created and data uploaded to it. Linking AutoLFADS script to your data First, make sure you are SSHed back into your server VM. Then, (inside the SSH window), navigate to the autolfads/pbt_opt directory with the following command. cd autolfads/pbt_opt Next, we want to edit pbt_script_multiVM.py , the AutoLFADS run script, to link it to our data. Still inside the SSH window and inside the autolfads/pbt_opt directory, open pbt_script_multiVM.py in a text editor of your choice. In this tutorial, we will do so in nano with the following command: nano pbt_script_multiVM.py A cheat sheet on using nano is available here . Now, you should see at the top something that looks like the following: First we need to set bucket_name, data_path, and run_path to point towards our bucket. Bucket_name refers to the name of the bucket we created, data_path to the folder where we hold our data, and run_path to the folder where our run output will be. Name can be set to any string we want our run to be called. In this tutorial, we'll modify these to be the following: bucket_name = 'autolfadsbucket' data_path = 'data' run_path = 'runs' name = 'tut_run' These changes to the script are necessary to link AutoLFADS to your data. However, there is also a variety of HPs in this script whose intial values, range of variance, and manner of pertubation by PBT can be edited. For information on explaining the various HPs and how to edit their initial values and ranges, go to the modifying HPs section in the Additional Information section. You can save your changes and exit your text editor.","title":"Set-up run parameters"},{"location":"run_params/#linking-autolfads-script-to-your-data","text":"First, make sure you are SSHed back into your server VM. Then, (inside the SSH window), navigate to the autolfads/pbt_opt directory with the following command. cd autolfads/pbt_opt Next, we want to edit pbt_script_multiVM.py , the AutoLFADS run script, to link it to our data. Still inside the SSH window and inside the autolfads/pbt_opt directory, open pbt_script_multiVM.py in a text editor of your choice. In this tutorial, we will do so in nano with the following command: nano pbt_script_multiVM.py A cheat sheet on using nano is available here . Now, you should see at the top something that looks like the following: First we need to set bucket_name, data_path, and run_path to point towards our bucket. Bucket_name refers to the name of the bucket we created, data_path to the folder where we hold our data, and run_path to the folder where our run output will be. Name can be set to any string we want our run to be called. In this tutorial, we'll modify these to be the following: bucket_name = 'autolfadsbucket' data_path = 'data' run_path = 'runs' name = 'tut_run' These changes to the script are necessary to link AutoLFADS to your data. However, there is also a variety of HPs in this script whose intial values, range of variance, and manner of pertubation by PBT can be edited. For information on explaining the various HPs and how to edit their initial values and ranges, go to the modifying HPs section in the Additional Information section. You can save your changes and exit your text editor.","title":"Linking AutoLFADS script to your data "},{"location":"setupAddInfo/","text":"Stopping, Starting VMs Leaving VMs running can lead to unintended high bills. VMs can be stopped, deleted, and starting in the compute engine by clicking the checkbox to the left of the VM and clicking start/stop/delete buttons at the top of the list of VMs. Cloud Shell Many of the commands needed to set-up infrastructure are done using Google Cloud Shell. While the commands needed are listed directly in the tutorial, further information on navigating/using cloud shell can be found here: https://cloud.google.com/shell/docs/using-cloud-shell . Typical commands used for an AutoLFADS run are entering directories with cd , copying git repos with git clone , and running shell scripts with sh . Choosing number of clients In essence, each client VM can have 2-3 workers running on it, and the more total workers means a wider search for optimal hyperparameters. Thus, the more client VMs we create when setting up a run, the wider search for optimal hyperparameters. In general for most mid-sized datasets, having 24 workers usually will lead to finding optimal HPs. In this tutorial, we create 8 client VMs, which allows use of 24 workers. This number can be adjusted if the dataset is larger or smaller. Allocating Clients Between Multiple Zones If your AutoLFADS run involves a significant number of different client machines, but you lack the GPU quota in a single region, you can allocate these over a number of regions. For instance, if you want to create 8 clients machines, but only have a regional quota of 4 GPUs in either region, then you can create 4 in us-central1-c and 4 in us-east1-c by running the following commands consecutively. sh machine_setup.sh clientc 4 us-central1-c and then sh machine_setup.sh cliente 4 us-east1-c Requesting additional GPU quota Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, 24-48 hours before doing a run, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater).","title":"Additional Information"},{"location":"setupAddInfo/#stopping-starting-vms","text":"Leaving VMs running can lead to unintended high bills. VMs can be stopped, deleted, and starting in the compute engine by clicking the checkbox to the left of the VM and clicking start/stop/delete buttons at the top of the list of VMs.","title":"Stopping, Starting VMs"},{"location":"setupAddInfo/#cloud-shell","text":"Many of the commands needed to set-up infrastructure are done using Google Cloud Shell. While the commands needed are listed directly in the tutorial, further information on navigating/using cloud shell can be found here: https://cloud.google.com/shell/docs/using-cloud-shell . Typical commands used for an AutoLFADS run are entering directories with cd , copying git repos with git clone , and running shell scripts with sh .","title":"Cloud Shell"},{"location":"setupAddInfo/#choosing-number-of-clients","text":"In essence, each client VM can have 2-3 workers running on it, and the more total workers means a wider search for optimal hyperparameters. Thus, the more client VMs we create when setting up a run, the wider search for optimal hyperparameters. In general for most mid-sized datasets, having 24 workers usually will lead to finding optimal HPs. In this tutorial, we create 8 client VMs, which allows use of 24 workers. This number can be adjusted if the dataset is larger or smaller.","title":"Choosing number of clients"},{"location":"setupAddInfo/#allocating-clients-between-multiple-zones","text":"If your AutoLFADS run involves a significant number of different client machines, but you lack the GPU quota in a single region, you can allocate these over a number of regions. For instance, if you want to create 8 clients machines, but only have a regional quota of 4 GPUs in either region, then you can create 4 in us-central1-c and 4 in us-east1-c by running the following commands consecutively. sh machine_setup.sh clientc 4 us-central1-c and then sh machine_setup.sh cliente 4 us-east1-c","title":"Allocating Clients Between Multiple Zones"},{"location":"setupAddInfo/#requesting-additional-gpu-quota","text":"Compute Engine enforces quota to prevent unforseen spikes in GPU usage. The quota enforces an upper bound on how many GPUs can be created in each zone. Thus, 24-48 hours before doing a run, you must make sure your quota allows you to have enough GPUs to run your client machines, and if not, request additional quota. Generally, we need to increase our quota of 1) # of GPUs, 2) # of global GPUs, and 3) # of global CPUs. In order to request quota, navigate to https://console.cloud.google.com/iam-admin/quotas , open up the 'Metric' drop down menu, de-select all by clicking 'none.' First, scroll down to find an appropriate GPU that you will be attaching to your virtual machines. The user can choose any GPU that suits their purpose; the default one used in this tutorial is NVIDIA K80 GPU. Note, the selected GPU works well as 'normal' type, not 'committed' (higher costs) or 'preemptible' (short-lived VMs). Select the chosen GPU, and then scroll down to the specific region you want to increase quota in. Once you select it, click the 'Edit Quotas,' and fill in the information. To follow this tutorial exactly, you need at least 4 NVIDA K80 GPUs in us-central1-c, and 4 NVIDIA K80 GPUs in us-east1-c. Next, we want to increase the number of global GPUs. You can deselect all metrics again, and then find GPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 8 (or greater). Finally, we want to increase the number of global CPUs. You can deselect all metrics again, and then find CPUs (all regions) . To follow this tutorial exactly, click edit quota and increase this to 64 (or greater).","title":"Requesting additional GPU quota"},{"location":"theory/","text":"This section details a general theoretical overview of AutoLFADS. We can generally think of AutoLFADS as a combination of LFADS with a hyperparameter tuning framework called population-based training, or PBT. In order to understand AutoLFADS, we will first look at LFADS and PBT independently. What is LFADS? Populations of neurons show latent, low-dimensional structure that is not apparent from studying individual neurons, and this latent structure exhibits dynamics - a set of rules that determine how the system evolves. LFADS (Latent Factor Analysis of Dynamical Systems) is a deep learning tool which models these neural population dynamics. What is PBT? The ability of LFADS to learn and train from data hinges on many values set prior to training, called hyperparameters (HPs). We can think of HPs as essentially the 'settings' of LFADS --- with improper settings, LFADS can train slowly or incompletely. However, it is difficult to know what the \"correct\" settings are, as the ideal HPs differs from dataset to dataset! In this way, LFADS with unoptimized hyperparameters will sometimes struggle to fit correctly to data. PBT is a framework that allows optimization of hyperparameters. Thus, LFADS run with PBT, or what we deem AutoLFADS, can arrive at HPs that are far more optimized and can lead to far better performance than LFADS by itself. How does PBT work? PBT optimizes HPs during training through what is called 'evolutionary optimization,' a process modeled after evolutionary strategies. First, instead of training a single deep-learning model (for instance, LFADS) to completion, PBT generates an entire population of models, where each member of the population is an individual deep-learning model with randomly initialized unique HPs and weights. For convenience, an individual deep learning model with its own HPs and weights is deemed a 'worker.' With PBT, the entire population of workers is trained in parallel. During training, weaker performing workers (models with suboptimal HPs and thus suboptimal weights) copy the HPs and weights of better performing workers, in code called exploit . After exploiting, the values of the copied HPs are slightly changed as a way to search for more optimal HPs, in code called explore . In this way, instead of workers with poorly optimized HPs floundering for an entire run, they instead steal the weights and HPs of the best performing worker and then search from there for even better optimized HPs. Thus, by the end of all the workers training, we end up with a strong performing best worker which has highly optimized HPs. What is AutoLFADS? AutoLFADS is the training of LFADS with a PBT framework for HP optimization. While LFADS will struggle in being applied to less-structured behaviors or brain areas not primarily governed by intrinsic dynamics, AutoLFADS with its HP optimization can achieve good performance in these diverse datasets.","title":"Theory"},{"location":"theory/#what-is-lfads","text":"Populations of neurons show latent, low-dimensional structure that is not apparent from studying individual neurons, and this latent structure exhibits dynamics - a set of rules that determine how the system evolves. LFADS (Latent Factor Analysis of Dynamical Systems) is a deep learning tool which models these neural population dynamics.","title":"What is LFADS?"},{"location":"theory/#what-is-pbt","text":"The ability of LFADS to learn and train from data hinges on many values set prior to training, called hyperparameters (HPs). We can think of HPs as essentially the 'settings' of LFADS --- with improper settings, LFADS can train slowly or incompletely. However, it is difficult to know what the \"correct\" settings are, as the ideal HPs differs from dataset to dataset! In this way, LFADS with unoptimized hyperparameters will sometimes struggle to fit correctly to data. PBT is a framework that allows optimization of hyperparameters. Thus, LFADS run with PBT, or what we deem AutoLFADS, can arrive at HPs that are far more optimized and can lead to far better performance than LFADS by itself.","title":"What is PBT?"},{"location":"theory/#how-does-pbt-work","text":"PBT optimizes HPs during training through what is called 'evolutionary optimization,' a process modeled after evolutionary strategies. First, instead of training a single deep-learning model (for instance, LFADS) to completion, PBT generates an entire population of models, where each member of the population is an individual deep-learning model with randomly initialized unique HPs and weights. For convenience, an individual deep learning model with its own HPs and weights is deemed a 'worker.' With PBT, the entire population of workers is trained in parallel. During training, weaker performing workers (models with suboptimal HPs and thus suboptimal weights) copy the HPs and weights of better performing workers, in code called exploit . After exploiting, the values of the copied HPs are slightly changed as a way to search for more optimal HPs, in code called explore . In this way, instead of workers with poorly optimized HPs floundering for an entire run, they instead steal the weights and HPs of the best performing worker and then search from there for even better optimized HPs. Thus, by the end of all the workers training, we end up with a strong performing best worker which has highly optimized HPs.","title":"How does PBT work?"},{"location":"theory/#what-is-autolfads","text":"AutoLFADS is the training of LFADS with a PBT framework for HP optimization. While LFADS will struggle in being applied to less-structured behaviors or brain areas not primarily governed by intrinsic dynamics, AutoLFADS with its HP optimization can achieve good performance in these diverse datasets.","title":"What is AutoLFADS?"}]}